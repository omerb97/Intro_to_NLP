{"cells":[{"cell_type":"code","execution_count":1,"id":"82746f00","metadata":{"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\n","[notice] A new release of pip available: 22.3.1 -> 23.0.1\n","[notice] To update, run: python3.10 -m pip install --upgrade pip\n"]}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2023-spring/lab0-1.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"]},{"cell_type":"code","execution_count":2,"id":"f54f4b4f","metadata":{"deletable":false,"editable":false},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"attachments":{},"cell_type":"markdown","id":"be8a643d","metadata":{"tags":["remove_for_latex"]},"source":["# Course 236299\n","\n","## Lab0-1: Tensors and vectorization"]},{"cell_type":"markdown","id":"bbbb4059","metadata":{},"source":["Many of the data-heavy approaches to NLP are enabled by advances in parallel processing that make what were once intractable computations practical. This notebook demonstrates the issue and the `torch` technologies that apply to it, especially the \"tensor\" data type.\n","\n","New bits of Python used for the first time in this lab, and which you may therefore want to review, include:\n","\n","* `assert`\n","* `globals`\n","* `len`\n","* `math.inf`\n","* `random.choices`\n","* `timeit.timeit`\n","* `torch.tensor`\n","* `torch.equal`\n","* `torch.rand`\n","* `torch.shape`\n","* `torch.transpose`\n","* `torch.view`\n","* `torch.zeros`\n","* `torch.zeros_like`"]},{"cell_type":"code","execution_count":3,"id":"e1cc93c0","metadata":{},"outputs":[],"source":["import torch\n","import random\n","\n","from timeit import timeit"]},{"cell_type":"markdown","id":"fcfd9130","metadata":{},"source":["Numeric vectors can be implemented in Python in many ways. Most directly, Python provides a built-in `list` data type, which we could use to implement a vector. Here, we generate a couple of example vectors as lists each containing 1000 integers between 0 and 99."]},{"cell_type":"code","execution_count":4,"id":"15742d2f","metadata":{},"outputs":[],"source":["a1 = random.choices(range(100), k=1000)\n","a2 = random.choices(range(100), k=1000)"]},{"cell_type":"markdown","id":"d7a44ee1","metadata":{},"source":["# An example: dot product\n","\n","The dot product of two vectors $v$ and $w$ is the sum of their componentwise product, $\\sum_i v_i \\cdot w_i$, which can be calculated with a simple for-loop."]},{"cell_type":"code","execution_count":5,"id":"06393378","metadata":{},"outputs":[],"source":["def dotproduct(v, w):\n","    assert len(v) == len(w)\n","    sum = 0\n","    for i in range(len(v)):\n","         sum += v[i] * w[i]\n","    return sum"]},{"cell_type":"code","execution_count":6,"id":"adcec1c5","metadata":{},"outputs":[{"data":{"text/plain":["2482075"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["dotproduct_result = dotproduct(a1, a2)\n","dotproduct_result"]},{"cell_type":"markdown","id":"56035b8f","metadata":{},"source":["We can test the efficiency of this approach to implementing vectors by computing a large dot product many times. (We use the `timeit` function that we imported from the `timeit` library above to return the time in seconds to perform 100,000 repetitions of the dot product computation.)"]},{"cell_type":"code","execution_count":7,"id":"7ac828de","metadata":{},"outputs":[{"data":{"text/plain":["'It took 5.0486171250013285 seconds.'"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["example_time = timeit('dotproduct(a1, a2)', number=100000, globals=globals())\n","f\"It took {example_time} seconds.\""]},{"cell_type":"markdown","id":"86d654f0","metadata":{},"source":["As it turns out, performing this vector computation is quite slow -- it probably took several seconds -- because the `for` loop over the list data structure computes sequentially. Instead, we can use a data type engineered especially for such vector and array computations to improve performance. Such data types include Python arrays, `numpy` arrays, and [`torch` tensors](https://pytorch.org/docs/stable/tensors.html). The latter are especially designed for the kinds of computations found in machine learning algorithms, so we will use them throughout the course. You can read (a lot) more about tensors in [the official documentation](https://pytorch.org/docs/stable/tensors.html).\n","\n","We construct a couple of one-dimensional tensors for the examples above."]},{"cell_type":"code","execution_count":8,"id":"5eeacc00","metadata":{},"outputs":[],"source":["t1 = torch.tensor(a1)\n","t2 = torch.tensor(a2)"]},{"cell_type":"markdown","id":"4b10c12d","metadata":{},"source":["# Tensor properties\n","\n","Tensors have four characteristics that are especially useful (but potentially confusing when you first start working with them):\n","\n","+ Componentwise operation: Many operations on tensors work component by component instead of all at once.\n","+ Broadcast: Operations can broadcast individual elements to each component of a tensor.\n","+ Reshaping: Tensors can be reshaped to present the same elements in different configurations.\n","+ Special operations: Tensors have methods implementing certain operations especially efficiently.\n","\n","We give examples of each."]},{"cell_type":"markdown","id":"c1e93769","metadata":{},"source":["## Componentwise operation\n","\n","When we add two tensors of the same shape with the `+` operator, the summation percolates down to the individual comonents. For example,"]},{"cell_type":"code","execution_count":9,"id":"16815009","metadata":{},"outputs":[{"data":{"text/plain":["tensor([5, 7, 9])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["a3 = [1, 2, 3]\n","a4 = [4, 5, 6]\n","t3 = torch.tensor(a3)\n","t4 = torch.tensor(a4)\n","\n","t3 + t4"]},{"cell_type":"markdown","id":"488db79c","metadata":{},"source":["This is quite different from, say, lists, which perform a completely different operation – concatenation – when summed with the `+` operator."]},{"cell_type":"code","execution_count":10,"id":"c09d9644","metadata":{},"outputs":[{"data":{"text/plain":["[1, 2, 3, 4, 5, 6]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["a3 + a4"]},{"cell_type":"markdown","id":"a6fd25dd","metadata":{},"source":["Similarly, we can compute the elementwise product of two tensors of the same shape."]},{"cell_type":"code","execution_count":11,"id":"bb616063","metadata":{},"outputs":[{"data":{"text/plain":["tensor([ 4, 10, 18])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["t3 * t4"]},{"cell_type":"markdown","id":"3237275f","metadata":{},"source":["## Broadcast\n","\n","Related, adding a scalar to a tensor \"broadcasts\" the scalar addition operation to each element."]},{"cell_type":"code","execution_count":12,"id":"b0998284","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([6, 7, 8])\n","tensor([6, 7, 8])\n"]}],"source":["print(t3 + 5)\n","print(5 + t3)"]},{"cell_type":"markdown","id":"3fdf0ca9","metadata":{},"source":["Again, compare with how lists work (or actually, don't work)."]},{"cell_type":"code","execution_count":13,"id":"0e7846f8","metadata":{"tags":["raises-exception"]},"outputs":[],"source":["# a3 + 5"]},{"cell_type":"markdown","id":"b579b9bb","metadata":{},"source":["## Reshaping\n","\n","Finally, tensors can be reshaped so that their elements appear in a different configuration. The `view` method is often used to carry out the reshaping. For instance, we start with the following 3 by 4 tensor."]},{"cell_type":"code","execution_count":14,"id":"8f0fe3f4","metadata":{},"outputs":[],"source":["t5 = torch.tensor([[11, 12, 13, 14],\n","                   [21, 22, 23, 24],\n","                   [31, 32, 33, 34]])"]},{"cell_type":"markdown","id":"5758eec4","metadata":{},"source":["We can tell that `t5` is a 3 by 4 tensor using the `shape` method."]},{"cell_type":"code","execution_count":15,"id":"2a7c1ac9","metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([3, 4])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["t5.shape"]},{"cell_type":"markdown","id":"6b7282c7","metadata":{},"source":["We can view the elements as a 4 by 3 tensor, or a 2 by 2 by 3 tensor, or a 3 by 1 by 4 tensor."]},{"cell_type":"code","execution_count":16,"id":"d7e9211e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[11, 12, 13],\n","        [14, 21, 22],\n","        [23, 24, 31],\n","        [32, 33, 34]])\n","tensor([[[11, 12, 13],\n","         [14, 21, 22]],\n","\n","        [[23, 24, 31],\n","         [32, 33, 34]]])\n","tensor([[[11, 12, 13, 14]],\n","\n","        [[21, 22, 23, 24]],\n","\n","        [[31, 32, 33, 34]]])\n"]}],"source":["print(t5.view(4, 3))\n","print(t5.view(2, 2, 3))\n","print(t5.view(3, 1, 4))"]},{"cell_type":"markdown","id":"996e4f2d","metadata":{},"source":["## Special operations\n","\n","Tensors have a large set of methods defined on them that work especially efficiently, for instance, taking the sum of the elements, or the minimum."]},{"cell_type":"code","execution_count":17,"id":"8cb73c09","metadata":{},"outputs":[{"data":{"text/plain":["tensor(270)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["t5.sum()"]},{"cell_type":"code","execution_count":18,"id":"6f301563","metadata":{},"outputs":[{"data":{"text/plain":["tensor(11)"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["t5.min()"]},{"cell_type":"markdown","id":"e85da53a","metadata":{},"source":["The `min` method takes the minimum over all the elements in the tensor. But we often want to take the minimum or maximum with respect to a particular dimension, returning a tensor of these optima. For example, given a two-dimensional tensor, to find the minimum for each row, we can take the `min` with respect to the second dimension. (We need to pass a `1` since dimensions are 0-indexed.) \n","\n","For some intuition (especially helpful as the number of dimensions gets higher and you lose the simple notion of \"rows\" and \"columns\"), think of the `.min` by dimension operation as collapsing that dimension - for example, if you take the `.min(1)` of a $4 \\times 5 \\times 6$ tensor, your output will be a $4 \\times 6$ tensor. \n","\n","The `min` method called in this way returns  a tensor of the various minimum values, along with the indices at which these minima occurred. We can extract the `values` if we are interested only in those. (Feel free to take a look at the `indices` part of the return value to see what that looks like. Does its value make sense?)"]},{"cell_type":"code","execution_count":19,"id":"6e919c1b","metadata":{},"outputs":[{"data":{"text/plain":["tensor([11, 21, 31])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["t5.min(1).values"]},{"cell_type":"markdown","id":"50a55a53","metadata":{"deletable":false,"editable":false},"source":["<!--\n","BEGIN QUESTION\n","name: max_val\n","-->\n","Find a function to find the maximum values for each column of a two-dimensional tensor."]},{"cell_type":"code","execution_count":24,"id":"299a5552","metadata":{},"outputs":[],"source":["# TODO -- Implement a function to return the max value of each column, stored in a 1-D tensor.\n","def max_col(v):\n","    return v.max(0).values"]},{"cell_type":"code","execution_count":25,"id":"7de77e1e","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"max_val\")"]},{"cell_type":"markdown","id":"46a605d2","metadata":{"deletable":false,"editable":false},"source":["# Vectorized dot product\n","\n","<!--\n","BEGIN QUESTION\n","name: dotprod\n","-->\n","Using these tensor techniques, and noting the examples above, reimplement a version of `dotproduct` that has no `for` loops. **Hint:** Your code should be *very short*."]},{"cell_type":"code","execution_count":30,"id":"e0572468","metadata":{},"outputs":[],"source":["# TODO -- Implement a vectorized dot product, which should be much faster.\n","def dotproduct_v(v1, v2):\n","    return torch.dot(torch.tensor(v1),torch.tensor(v2))"]},{"cell_type":"code","execution_count":31,"id":"a10a22b4","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"dotprod\")"]},{"cell_type":"markdown","id":"f815f48a","metadata":{},"source":["This vectorized version should be *much* faster, perhaps a couple of orders of magnitude."]},{"cell_type":"code","execution_count":32,"id":"dc5aad03","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/yz/knd8kwrs5ds_sj63j6tp1_800000gn/T/ipykernel_55133/2977661956.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  return torch.dot(torch.tensor(v1),torch.tensor(v2))\n"]},{"data":{"text/plain":["0.05643012499785982"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["timeit('dotproduct_v(t1, t2)', number=10000, globals=globals())"]},{"cell_type":"markdown","id":"5b875100","metadata":{},"source":["# Vectorized computations over multidimensional tensors\n","\n","As you saw above, tensors aren't limited to one dimension, and these same vectorization tricks apply to multidimensional tensors. In fact, because of vectorization, we can specify computations over multidimensional tensors that look like the kinds of things you've seen in linear algebra. "]},{"cell_type":"markdown","id":"20ee61b2","metadata":{},"source":["#### An example: all-paths shortest path\n","\n","<img src=\"https://github.com/nlp-course/data/raw/master/Resources/small-graph.png\" width=200 align=right />\n","As a concrete example to give you some practice, consider the algorithm for computing shortest paths in a graph of $n$ nodes. We'll represent the graph as an $n \\times n$ matrix $A$ where $A_{ij}$ is the distance from node $i$ to node $j$. (Thus, this is a directed graph, and the distances needn't be symmetric.) We'll assume that the distance from a node to itself is zero. Here's an example, with distances that happen to be symmetric:"]},{"cell_type":"code","execution_count":33,"id":"2798a375","metadata":{},"outputs":[],"source":["from math import inf\n","distances = torch.tensor(\n","              [[0, 1,   2, 6],\n","               [1, 0,   2, inf],\n","               [2, 2,   0, 3],\n","               [6, inf, 3, 0]])"]},{"cell_type":"markdown","id":"70fef18f","metadata":{},"source":["(We use `inf` for an infinite distance, that is, for nodes that are not connected with an edge.) \n","\n","In this graph, the distance from node 0 to node 3 is 6, but by going through node 2, we can shorten the path to 5. In general, we define [the *minplus* operation](https://en.wikipedia.org/wiki/Min-plus_matrix_multiplication) ($\\star$) on two square matrices $A$ (of shape $m \\times n$) and $B$ (of shape $n \\times p$):\n","$$(A \\star B)_{ij} = \\min_k A_{ik} + B_{kj}$$\n","As a special case, if $A$ and $B$ are two distance graphs over the same nodes (but perhaps with different distances), then $A \\star B$ is the graph that shows the best way to get from one node to another by traversing an edge from the first graph $A$ and then an edge from the second graph $B$.\n","\n","Here's an implementation of this operation `minplus` using `for` loops."]},{"cell_type":"code","execution_count":34,"id":"b129cf9e","metadata":{},"outputs":[],"source":["def minplus_loop(A, B):\n","    (arows, acols), (brows, bcols) = A.size(), B.size()\n","    assert acols == brows\n","\n","    R = torch.zeros(arows, bcols)\n","    for i in range(arows):\n","        for j in range(bcols):\n","            min = torch.tensor(inf)\n","            for k in range(acols):\n","                if A[i,k] + B[k,j] < min:\n","                    min = A[i,k] + B[k,j]\n","            R[i,j] = min\n","    return R"]},{"cell_type":"markdown","id":"1d5abe1a","metadata":{},"source":["Let's test it out on a small rectangular matrix."]},{"cell_type":"code","execution_count":35,"id":"8d6b15a6","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[2., 5.],\n","        [5., 8.]])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["test = torch.tensor([[1, 2, 3], [4, 5, 6]])\n","minplus_loop(test, test.transpose(0, 1))"]},{"cell_type":"markdown","id":"c2254e59","metadata":{},"source":["Using this, we can compute some better ways of getting among the nodes in the`distances` graph. For paths of length at most 2, we can compute"]},{"cell_type":"code","execution_count":36,"id":"6eb69efc","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0., 1., 2., 5.],\n","        [1., 0., 2., 5.],\n","        [2., 2., 0., 3.],\n","        [5., 5., 3., 0.]])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["minplus_loop(distances, distances)"]},{"cell_type":"markdown","id":"5b6e0342","metadata":{},"source":["Notice that in this graph, the distance from node 0 to node 3 is now only 5 (not 6), and there are also now paths between nodes 1 and 3.\n","\n","We can compute the minimum distance between any two nodes by repeating this minplus process until no further distance reductions are possible and the graph has reached a stable point (the so-called \"fixpoint\"). We return as a Python tuple both the fixpoint graph and the number of rounds of `minplus` that were needed to reach it."]},{"cell_type":"code","execution_count":37,"id":"6bf02609","metadata":{},"outputs":[],"source":["def minplus_fp(X):\n","    rounds = 0\n","    lastY = torch.zeros_like(X)\n","    Y = X\n","    while not(torch.equal(Y, lastY)):\n","        lastY = Y\n","        Y = minplus_loop(Y, Y)\n","        rounds += 1\n","    return Y, rounds"]},{"cell_type":"code","execution_count":38,"id":"e6914dcc","metadata":{},"outputs":[{"data":{"text/plain":["(tensor([[0., 1., 2., 5.],\n","         [1., 0., 2., 5.],\n","         [2., 2., 0., 3.],\n","         [5., 5., 3., 0.]]),\n"," 2)"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["minplus_fp(distances)"]},{"cell_type":"markdown","id":"12a125cf","metadata":{},"source":["It turns out that after just the two rounds, the fixpoint is reached.\n","\n","> **Digression:** The complexity of `minplus` as implemented is $O(n^3)$, and the fixpoint computation may need up to $\\log n$ calls to `minplus` to converge, so the overall complexity is $O(n^3 \\log n)$. More efficient algorithms are known, especially the Floyd-Warshall algorithm for the all-paths versions and Dijkstra's algorithm for the single-source version. But algorithmic efficiency is not our main aim here.\n","\n","Let's try a bigger example, a graph with 10 nodes."]},{"cell_type":"code","execution_count":39,"id":"f34fe852","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.0000e+00, 4.2505e-01, 3.0965e-01, 6.4090e-01, 6.7334e-01, 9.8430e-02,\n","         5.7320e-01, 2.0361e-01, 3.8456e-01, 8.2615e-01],\n","        [5.6891e-01, 0.0000e+00, 2.7143e-02, 3.2502e-01, 3.5041e-02, 7.5075e-01,\n","         8.6859e-01, 7.6955e-01, 5.6873e-02, 3.6618e-01],\n","        [6.1322e-01, 3.7065e-01, 0.0000e+00, 5.1773e-01, 1.3566e-01, 6.7963e-02,\n","         7.8901e-01, 6.9120e-01, 4.6191e-01, 1.3091e-01],\n","        [3.0726e-01, 3.9574e-01, 4.7901e-01, 0.0000e+00, 5.0800e-01, 1.3123e-01,\n","         8.8808e-01, 4.8825e-01, 2.6500e-01, 1.7050e-01],\n","        [1.6970e-02, 4.3320e-01, 4.6135e-01, 9.9055e-01, 0.0000e+00, 8.2338e-01,\n","         1.6053e-01, 9.8679e-01, 9.2689e-01, 6.2030e-01],\n","        [7.1788e-01, 5.3287e-05, 9.7994e-01, 7.4450e-01, 7.5210e-01, 0.0000e+00,\n","         7.6283e-01, 3.9656e-01, 6.4609e-02, 9.1956e-01],\n","        [5.7265e-01, 2.8435e-01, 7.2664e-01, 3.1332e-01, 9.4936e-01, 8.9244e-01,\n","         0.0000e+00, 4.3847e-01, 9.5563e-02, 6.8260e-01],\n","        [1.9799e-01, 6.8484e-01, 6.7747e-01, 3.1104e-01, 5.9969e-01, 7.4987e-01,\n","         5.8664e-01, 0.0000e+00, 9.9086e-01, 9.9741e-02],\n","        [4.5784e-02, 9.2032e-01, 4.6127e-01, 5.2653e-01, 5.2679e-02, 8.1535e-01,\n","         9.4870e-02, 5.9057e-01, 0.0000e+00, 4.1682e-01],\n","        [2.6637e-01, 5.4470e-01, 8.7034e-01, 6.5232e-01, 4.7979e-01, 8.1072e-01,\n","         7.0979e-02, 2.1164e-01, 4.9653e-01, 0.0000e+00]])"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["def random_square_tensor(size):\n","    X = torch.rand(size, size)\n","    for i in range(size):\n","        X[i, i] = 0 \n","    return X\n","\n","X = random_square_tensor(10)\n","X"]},{"cell_type":"code","execution_count":40,"id":"598c4007","metadata":{},"outputs":[{"data":{"text/plain":["(tensor([[0.0000e+00, 9.8483e-02, 1.2563e-01, 4.2350e-01, 1.3352e-01, 9.8430e-02,\n","          2.5023e-01, 2.0361e-01, 1.5536e-01, 2.5654e-01],\n","         [5.2011e-02, 0.0000e+00, 2.7143e-02, 3.2502e-01, 3.5041e-02, 9.5106e-02,\n","          1.5174e-01, 2.5562e-01, 5.6873e-02, 1.5805e-01],\n","         [1.2003e-01, 6.8016e-02, 0.0000e+00, 3.9303e-01, 1.0306e-01, 6.7963e-02,\n","          2.0189e-01, 3.2364e-01, 1.2489e-01, 1.3091e-01],\n","         [1.8329e-01, 1.3128e-01, 1.5843e-01, 0.0000e+00, 1.6632e-01, 1.3123e-01,\n","          2.4148e-01, 3.8214e-01, 1.8816e-01, 1.7050e-01],\n","         [1.6970e-02, 1.1545e-01, 1.4260e-01, 4.4047e-01, 0.0000e+00, 1.1540e-01,\n","          1.6053e-01, 2.2058e-01, 1.7233e-01, 2.7351e-01],\n","         [5.2064e-02, 5.3287e-05, 2.7196e-02, 3.2507e-01, 3.5094e-02, 0.0000e+00,\n","          1.5180e-01, 2.5568e-01, 5.6927e-02, 1.5811e-01],\n","         [1.4135e-01, 2.3983e-01, 2.6697e-01, 3.1332e-01, 1.4824e-01, 2.3978e-01,\n","          0.0000e+00, 3.4496e-01, 9.5563e-02, 3.9788e-01],\n","         [1.9799e-01, 2.9648e-01, 3.2362e-01, 3.1104e-01, 3.1896e-01, 2.9642e-01,\n","          1.7072e-01, 0.0000e+00, 2.6628e-01, 9.9741e-02],\n","         [4.5784e-02, 1.4427e-01, 1.7141e-01, 4.0819e-01, 5.2679e-02, 1.4421e-01,\n","          9.4870e-02, 2.4940e-01, 0.0000e+00, 3.0232e-01],\n","         [2.1233e-01, 3.1081e-01, 3.3795e-01, 3.8430e-01, 2.1922e-01, 3.1076e-01,\n","          7.0979e-02, 2.1164e-01, 1.6654e-01, 0.0000e+00]]),\n"," 4)"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["minplus_fp(X)"]},{"cell_type":"markdown","id":"5e164408","metadata":{},"source":["Now let's talk about the \"loop\"-y implementation of the `minplus` function. If we're a little cleverer, we can use list comprehensions to hide the computation of the minimum, but we're still doing the whole computation sequentially."]},{"cell_type":"code","execution_count":41,"id":"a764aafa","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[2., 5.],\n","        [5., 8.]])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["def minplus_loop2(A, B):\n","    (arows, acols), (brows, bcols) = A.size(), B.size()\n","    assert acols == brows\n","\n","    R = torch.zeros(arows, bcols)\n","    assert acols == brows\n","    for i in range(arows):\n","        for j in range(bcols):\n","            R[i,j] = min([A[i,k] + B[k,j] for k in range(acols)])\n","    return R\n","\n","minplus_loop2(test, test.transpose(0,1))"]},{"cell_type":"markdown","id":"ab472df8","metadata":{},"source":["The `torch`-y way to perform this computation is to rely on vectorized computations. Doing so is a bit tricky however. We need to reshape the matrices a bit. We start by turning the two-dimensional $A$ matrix from a $m \\times n$ matrix (rows by columns, which may differ in the general case) into a three-dimensional $m \\times 1 \\times n$ matrix. Here's the result of that operation on the $4 \\times 4$ `distances` matrix, using [the torch `view` method](https://pytorch.org/docs/stable/tensor_view.html). We'll refer to the reshaped matrix below as the $A$ matrix henceforth. It will now be a tensor with 4 elements, each of which has 1 element, each of which has 4 elements (that are scalars)."]},{"cell_type":"code","execution_count":42,"id":"ff5c65d1","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[0., 1., 2., 6.]],\n","\n","        [[1., 0., 2., inf]],\n","\n","        [[2., 2., 0., 3.]],\n","\n","        [[6., inf, 3., 0.]]])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["A = distances.view(4, 1, 4)\n","A"]},{"cell_type":"markdown","id":"7e42b442","metadata":{},"source":["Now each row in the matrix contains a single element, a vector that corresponds to the column in the original. We'll do a similar operation on $B$ of shape $n \\times p$ (again, the `distances` matrix in our example), but this time reshaping the matrix to be $1 \\times p \\times n$. We'll refer to the reshaped matrix below as $B$. It will now be a tensor with just 1 element, and that element contains 4 elements, each of which contains 4 elements."]},{"cell_type":"code","execution_count":43,"id":"b976dd6d","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[0., 1., 2., 6.],\n","         [1., 0., 2., inf],\n","         [2., 2., 0., 3.],\n","         [6., inf, 3., 0.]]])"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["B = distances.view(1, 4, 4)\n","B"]},{"cell_type":"markdown","id":"4157876d","metadata":{},"source":["Now if we add these two matrices componentwise, what do we get? Each of the four 1 $\\times$ 4 elements in $A$ corresponds to the same single 4 $\\times$ 4 element in $B$, so they'll be added componentwise; that single element in $B$ will be \"broadcast\" to each of the rows in $A$. Thus, the first element in $A$ (`[[0., 1., 2., 6.]]` in the example) is to be added to the single element in $B$ (`[[0., 1., 2., 6.], [1., 0., 2., inf], [2., 2., 0., 3.], [5., inf, 3., 0.]]` in the example). How does this work? Going one more level in, we now have that the single 4-element element `[0., 1., 2., 6.]` in $A$ corresponds to (and will be broadcast to) each of the four 4-element elements in the single element in $B$ (the first of which is also `[0., 1., 2., 6.]`). Now, we've reached a set of elements that are all the same size as the element to be broadcast, so they are added elementwise, yielding `[0., 2., 4., 12.]` as the first summed 4-element element. The same thing happens for each of the other three elements in the reshaped $B$ matrix element. Zooming out one step, we repeat this procedure for each of the other three 1 $\\times$ 4 elements of $A$, broadcasting them onto the single 4 $\\times$ 4 element of $B$. When all is said and done, we'll have an $m \\times p \\times n$ matrix. Each outermost element of the summed matrix corresponds to a broadcast of a 1 $\\times$ 4 element of $A$ onto the 4 $\\times$ 4 element of $B$.\n","\n","As another way to understand this process, we want to compute the elementwise sum of a matrix $A$ of size 4 $\\times$ 1 $\\times$ 4 with another matrix $B$ of size 1 $\\times$ 4 $\\times$ 4. If both matrices were of size 4 $\\times$ 4 $\\times$ 4, there would be no problem. However, the first dimension of $A$ is of size 4 whereas the first dimension of $B$ is of size 1. Therefore, we repeat $B$ 4 times along the first dimension, expanding it into a matrix of size 4 $\\times$ 4 $\\times$ 4. Now the first dimension works out, but the second dimension of $A$ is of size 1 whereas the second dimension of $B$ is of size 4, so we also need to repeat $A$ 4 times along the second dimension, expanding it into a matrix of 4 $\\times$ 4 $\\times$ 4. Now both $A$ and $B$ have been expanded into size 4 $\\times$ 4 $\\times$ 4, and we can directly compute their elementwise sum. This hidden expanding operation is exactly the broadcasting we've referred to above."]},{"cell_type":"code","execution_count":44,"id":"5753913c","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[ 0.,  2.,  4., 12.],\n","         [ 1.,  1.,  4., inf],\n","         [ 2.,  3.,  2.,  9.],\n","         [ 6., inf,  5.,  6.]],\n","\n","        [[ 1.,  1.,  4., inf],\n","         [ 2.,  0.,  4., inf],\n","         [ 3.,  2.,  2., inf],\n","         [ 7., inf,  5., inf]],\n","\n","        [[ 2.,  3.,  2.,  9.],\n","         [ 3.,  2.,  2., inf],\n","         [ 4.,  4.,  0.,  6.],\n","         [ 8., inf,  3.,  3.]],\n","\n","        [[ 6., inf,  5.,  6.],\n","         [ 7., inf,  5., inf],\n","         [ 8., inf,  3.,  3.],\n","         [12., inf,  6.,  0.]]])"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["summed = distances.view(4, 1, 4) + distances.view(1, 4, 4)\n","summed"]},{"cell_type":"markdown","id":"07f08343","metadata":{},"source":["How does this matrix translate to the $A_{ik} + B_{kj}$ additions we did before? To illustrate by example (indexing starting from 0), let's think about $(A \\star B)_{1, 2} = \\min_k A_{1, k} + B_{k, 2}$, where $A$ and $B$ now refer to the original 4 $\\times$ 4 `distances` matrix that we started with. The $A_{1, k}$s would be found in the second element of `summed`, which is where we broadcasted the second (index 1) row of $A$. Since this matrix is symmetric, the $B_{k, 2}$'s would be equivalent to the $B_{2, k}$'s, which would be found in the second row of each element of `summed`. Taking these two pieces of information together, we then look at the second row of the second element of `summed`. Indeed, $A_{1, 0} + B_{0, 2} = A_{1, 0} + B_{2, 0}$ would be the first element in the third row of the second element of `summed`, and this is true for each $k$. This tells us that to find our `minplus` result, we just need to find the minimum of each row of every element of `summed` (using the `min` method), and there are $4 \\cdot 4$ such rows. Note that this solution only works for symmetric matrices - for non-symmetric matrices, you'll need to transpose `distances` before you reshape it into a $1 \\times 4 \\times 4$ tensor."]},{"cell_type":"markdown","id":"ca43ebb3","metadata":{"deletable":false,"editable":false},"source":["<!--\n","BEGIN QUESTION\n","name: minplus_v_example\n","-->\n","Calculate the result of the `minplus` by performing appropriate operations on `summed` to yield a tensor that should be identical to the `minplus_loop(distances, distances)` example above."]},{"cell_type":"code","execution_count":58,"id":"e75de3b0","metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0., 1., 2., 5.],\n","        [1., 0., 2., 5.],\n","        [2., 2., 0., 3.],\n","        [5., 5., 3., 0.]])"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["#TODO\n","result = summed.min(2).values\n","result"]},{"cell_type":"code","execution_count":59,"id":"d10a547f","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"minplus_v_example\")"]},{"cell_type":"markdown","id":"bd909b62","metadata":{"deletable":false,"editable":false},"source":["Now that you've seen an example of how the matrices can be reshaped and operated on to implement the `minplus` operation, write a function `minplus_v` (a \"vectorized\" version of `minplus_loop`) that computes the minplus of two rectangular matrices without using any looping constructs.\n","<!--\n","BEGIN QUESTION\n","name: minplus_v\n","-->"]},{"cell_type":"code","execution_count":60,"id":"d9ceb532","metadata":{},"outputs":[],"source":["#TODO\n","def minplus_v(A, B):\n","    (arows, acols), (brows, bcols) = A.size(), B.size()\n","    newSummed = A.view(arows, 1, acols) + B.view(1, brows, bcols)\n","    return newSummed.min(2).values"]},{"cell_type":"code","execution_count":61,"id":"e92ef698","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"minplus_v\")"]},{"cell_type":"markdown","id":"f8f7e323","metadata":{},"source":["Finally, we'll use your vectorized `minplus_v` to implement a vectorized fixpoint calculation, and test the relative speeds."]},{"cell_type":"code","execution_count":62,"id":"aefba87e","metadata":{},"outputs":[],"source":["def minplus_vfp(X):\n","    rounds = 0\n","    lastY = torch.zeros_like(X)\n","    Y = X\n","    while not(torch.equal(Y, lastY)):\n","        lastY = Y\n","        Y = minplus_v(Y, Y)\n","        rounds += 1\n","    return Y, rounds"]},{"cell_type":"code","execution_count":63,"id":"ee9b6b61","metadata":{},"outputs":[{"data":{"text/plain":["(tensor([[0., 1., 2., 5.],\n","         [1., 0., 2., 5.],\n","         [2., 2., 0., 3.],\n","         [5., 5., 3., 0.]]),\n"," 2)"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["minplus_vfp(distances)"]},{"cell_type":"code","execution_count":64,"id":"45e54531","metadata":{},"outputs":[{"data":{"text/plain":["1.7197973339934833"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["example = random_square_tensor(20)\n","\n","timeit('minplus_fp(example)', number=10, globals=globals())"]},{"cell_type":"code","execution_count":65,"id":"914d30af","metadata":{},"outputs":[{"data":{"text/plain":["0.004676292002841365"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["timeit('minplus_vfp(example)', number=10, globals=globals())"]},{"cell_type":"markdown","id":"aca49fd4","metadata":{},"source":["If you've implemented `minplus_v` correctly, the efficiency difference should be striking. This kind of engineered improvement is the difference between a computation taking a day and one taking a minute."]},{"cell_type":"markdown","id":"822ffa83","metadata":{},"source":["# Submission Instructions\n","\n","This lab should be submitted to Gradescope at <https://rebrand.ly/lab0-1-submit>, which will be made available some time before the due date.\n","\n","Make sure that you have passed all public tests by running `grader.check_all()` below before submitting. Note that there are hidden tests on Gradescope, the results of which will be revealed after the submission deadline."]},{"cell_type":"markdown","id":"8697bf5d","metadata":{},"source":["# End of lab 0-1"]},{"cell_type":"markdown","id":"e963b13f","metadata":{"deletable":false,"editable":false},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","execution_count":66,"id":"7376cb1b","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["<p><strong>dotprod:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>max_val:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>minplus_v:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>minplus_v_example:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n"],"text/plain":["dotprod:\n","\n","    All tests passed!\n","    \n","\n","max_val:\n","\n","    All tests passed!\n","    \n","\n","minplus_v:\n","\n","    All tests passed!\n","    \n","\n","minplus_v_example:\n","\n","    All tests passed!\n","    \n"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["grader.check_all()"]}],"metadata":{"celltoolbar":"Tags","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8 (main, Oct 21 2022, 22:22:30) [Clang 14.0.0 (clang-1400.0.29.202)]"},"title":"CS236299 Lab 0-1: Tensors and vectorization","vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":5}
