{"cells":[{"cell_type":"code","execution_count":3,"id":"8cb89b74","metadata":{"collapsed":true,"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2023-spring/project2.git .tmp\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"]},{"cell_type":"code","execution_count":4,"id":"a4cf5e65","metadata":{"deletable":false,"editable":false},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"cell_type":"raw","id":"20f79c2f","metadata":{"jupyter":{"source_hidden":true}},"source":["%%latex\n","\\newcommand{\\vect}[1]{\\mathbf{#1}}\n","\\newcommand{\\cnt}[1]{\\sharp(#1)}\n","\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\newcommand{\\softmax}{\\operatorname{softmax}}\n","\\newcommand{\\Prob}{\\Pr}\n","\\newcommand{\\given}{\\,|\\,}"]},{"attachments":{},"cell_type":"markdown","id":"e855b192","metadata":{"jupyter":{"source_hidden":true}},"source":["$$\n","\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n","\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n","\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\renewcommand{\\softmax}{\\operatorname{softmax}}\n","\\renewcommand{\\Prob}{\\Pr}\n","\\renewcommand{\\given}{\\,|\\,}\n","$$"]},{"attachments":{},"cell_type":"markdown","id":"b72862de","metadata":{"colab_type":"text","tags":["remove_for_latex"]},"source":["# 236299 - Introduction to Natural Language Processing\n","## Project 2: Sequence labeling â€“ The slot filling task"]},{"attachments":{},"cell_type":"markdown","id":"9ab29c9d","metadata":{"colab_type":"text"},"source":["# Introduction\n","\n","The second segment of the project involves a sequence labeling task, in which the goal is to label the tokens in a text. Many NLP tasks have this general form. Most famously is the task of _part-of-speech labeling_ as you explored in lab 2-4, where the tokens in a text are to be labeled with their part of speech (noun, verb, preposition, etc.). In this project segment, however, you'll use sequence labeling to implement a system for filling the slots in a template that is intended to describe the meaning of an ATIS query. For instance, the sentence \n","\n","    What's the earliest arriving flight between Boston and Washington DC?\n","    \n","might be associated with the following slot-filled template: \n","\n","    flight_id\n","        fromloc.cityname: boston\n","        toloc.cityname: washington\n","        toloc.state: dc\n","        flight_mod: earliest arriving\n","    \n","You may wonder how this task is a sequence labeling task. We label each word in the source sentence with a tag taken from a set of tags that correspond to the slot-labels. For each slot-label, say `flight_mod`, there are two tags: `B-flight_mod` and `I-flight_mod`. These are used to mark the beginning (B) or interior (I) of a phrase that fills the given slot. In addition, there is a tag for other (O) words that are not used to fill any slot. (This technique is thus known as IOB encoding.) Thus the sample sentence would be labeled as follows:\n","\n","| Token   | Label    |\n","| :------ | :----- | \n","| `BOS` | `O` |\n","| `what's` | `O` |\n","| `the` | `O` |\n","| `earliest` | `B-flight_mod` |\n","| `arriving` | `I-flight_mod` |\n","| `flight` | `O` |\n","| `between` | `O` |\n","| `boston` | `B-fromloc.city_name` |\n","| `and` | `O` |\n","| `washington` | `B-toloc.city_name` |\n","| `dc` | `B-toloc.state_code` |\n","| `EOS` | `O` |\n","\n","> See below for information about the `BOS` and `EOS` tokens. \n","\n","The template itself is associated with the question type for the sentence, perhaps as recovered from the sentence in the last project segment.\n","\n","In this segment, you'll implement three methods for sequence labeling: a hidden Markov model (HMM) and two recurrent neural networks, a simple RNN and a long short-term memory network (LSTM). By the end of this homework, you should have grasped the pros and cons of the statistical and neural approaches.\n","\n","## Goals\n","\n","1. Implement an HMM-based approach to sequence labeling.\n","2. Implement an RNN-based approach to sequence labeling.\n","3. Implement an LSTM-based approach to sequence labeling.\n","4. Compare the performances of HMM and RNN/LSTM with different amounts of training data. Discuss the pros and cons of the HMM approach and the neural approach."]},{"attachments":{},"cell_type":"markdown","id":"151d9b8a","metadata":{"colab_type":"text"},"source":["## Setup"]},{"cell_type":"code","execution_count":5,"id":"0dc9c767","metadata":{"colab":{},"colab_type":"code","deletable":false,"editable":false},"outputs":[],"source":["import copy\n","import math\n","import matplotlib.pyplot as plt\n","import random\n","import csv\n","\n","import wget\n","import torch\n","import torch.nn as nn\n","import datasets\n","\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.pre_tokenizers import WhitespaceSplit\n","from tokenizers.processors import TemplateProcessing\n","from tokenizers import normalizers\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from transformers import PreTrainedTokenizerFast\n","\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":6,"id":"cdc953df","metadata":{"colab":{},"colab_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]}],"source":["# Set random seeds\n","seed = 1234\n","random.seed(seed)\n","torch.manual_seed(seed)\n","\n","# GPU check, sets runtime type to \"GPU\" where available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"attachments":{},"cell_type":"markdown","id":"e454e512","metadata":{"colab_type":"text"},"source":["## Loading data\n","\n","We download the ATIS dataset, already presplit into training, validation (dev), and test sets."]},{"cell_type":"code","execution_count":7,"id":"0d83def1","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["# Prepare to download needed data\n","def download_if_needed(filename, source='./', dest='./'):\n","    os.makedirs(data_path, exist_ok=True) # ensure destination\n","    os.path.exists(f\"./{dest}{filename}\") or wget.download(source + filename, out=dest)\n","\n","source_path = \"https://raw.githubusercontent.com/nlp-course/data/master/ATIS/\"\n","data_path = \"data/\"\n","\n","# Download files\n","for filename in [\"atis.train.txt\",\n","                 \"atis.dev.txt\",\n","                 \"atis.test.txt\"\n","                ]:\n","    download_if_needed(filename, source_path, data_path)"]},{"attachments":{},"cell_type":"markdown","id":"743a96fc","metadata":{"colab_type":"text"},"source":["## Data preprocessing\n","\n","We again use `datasets` and `tokenizers` to load data and convert words to indices in the vocabulary.\n","\n","We treat words occurring fewer than three times in the training data as _unknown words_. They'll be replaced by the unknown word type `[UNK]`."]},{"cell_type":"code","execution_count":8,"id":"6d70b06f","metadata":{},"outputs":[],"source":["for split in ['train', 'dev', 'test']:\n","    in_file = f'data/atis.{split}.txt'\n","    out_file = f'data/atis.{split}.csv'\n","    \n","    with open(in_file, 'r') as f_in:\n","        with open(out_file, 'w') as f_out:\n","            text, tag = [], []\n","            writer = csv.writer(f_out)\n","            writer.writerow(('text','tag'))\n","            for line in f_in:\n","                if line.strip() == '':\n","                    writer.writerow((' '.join(text), ' '.join(tag)))\n","                    text, tag = [], []\n","                else:\n","                    token, label = line.split('\\t')\n","                    text.append(token)\n","                    tag.append(label.strip())"]},{"attachments":{},"cell_type":"markdown","id":"754b0af7","metadata":{},"source":["Let's take a look at what each data file looks like."]},{"cell_type":"code","execution_count":9,"id":"bba1b055","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["text,tag\n","BOS what is the cost of a round trip flight from pittsburgh to atlanta beginning on april twenty fifth and returning on may sixth EOS,O O O O O O O B-round_trip I-round_trip O O B-fromloc.city_name O B-toloc.city_name O O B-depart_date.month_name B-depart_date.day_number I-depart_date.day_number O O O B-return_date.month_name B-return_date.day_number O\n","BOS now i need a flight leaving fort worth and arriving in denver no later than 2 pm next monday EOS,O O O O O O O B-fromloc.city_name I-fromloc.city_name O O O B-toloc.city_name B-arrive_time.time_relative I-arrive_time.time_relative I-arrive_time.time_relative B-arrive_time.time I-arrive_time.time B-arrive_date.date_relative B-arrive_date.day_name O\n","BOS i need to fly from kansas city to chicago leaving next wednesday and returning the following day EOS,O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O B-depart_date.date_relative B-depart_date.day_name O O B-return_date.date_relative I-return_date.date_relative I-return_date.date_relative O\n","BOS what is the meaning of meal code s EOS,O O O O O O B-meal_code I-meal_code I-meal_code O\n","BOS show me all flights from denver to pittsburgh which serve a meal for the day after tomorrow EOS,O O O O O O B-fromloc.city_name O B-toloc.city_name O O O B-meal O B-depart_date.today_relative I-depart_date.today_relative I-depart_date.today_relative I-depart_date.today_relative O\n","BOS show me all us air flights from atlanta to denver for the day after tomorrow EOS,O O O O B-airline_name I-airline_name O O B-fromloc.city_name O B-toloc.city_name O B-depart_date.today_relative I-depart_date.today_relative I-depart_date.today_relative I-depart_date.today_relative O\n","BOS list the nonstop flights early tuesday morning from dallas to atlanta EOS,O O O B-flight_stop O B-arrive_time.period_mod B-arrive_date.day_name B-arrive_time.period_of_day O B-fromloc.city_name O B-toloc.city_name O\n","BOS show me the flights from st. petersburg to toronto that arrive early in the morning EOS,O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O O B-arrive_time.period_mod O O B-arrive_time.period_of_day O\n","BOS i need a listing of flights from new york city to montreal canada departing thursday in the morning EOS,O O O O O O O O B-fromloc.city_name I-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.country_name O B-depart_date.day_name O O B-depart_time.period_of_day O\n"]}],"source":["shell('head \"data/atis.train.csv\"')"]},{"attachments":{},"cell_type":"markdown","id":"673e157a","metadata":{},"source":["We use `datasets` to prepare the data."]},{"cell_type":"code","execution_count":10,"id":"6ce95303","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration default-74137e384f73c9c4\n"]},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset csv/default to /Users/omerbareket/.cache/huggingface/datasets/csv/default-74137e384f73c9c4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e9208c494db6469cae287404c34e13fe","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1f9f912f690c488187d553b5fd0f6fe4","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"870a1fc379fa4fe8aac764ae3efea04f","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n","  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a5bf11a1133442089f8da843ca19f9f","version_major":2,"version_minor":0},"text/plain":["Generating val split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n","  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fc3776070c84857a8373ca35243485e","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset csv downloaded and prepared to /Users/omerbareket/.cache/huggingface/datasets/csv/default-74137e384f73c9c4/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n","  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26255574a3054f42a45d653a1065f45f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'tag'],\n","        num_rows: 4274\n","    })\n","    val: Dataset({\n","        features: ['text', 'tag'],\n","        num_rows: 572\n","    })\n","    test: Dataset({\n","        features: ['text', 'tag'],\n","        num_rows: 586\n","    })\n","})"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["atis = load_dataset('csv', data_files={'train':'data/atis.train.csv', \\\n","                                       'val': 'data/atis.dev.csv', \\\n","                                       'test': 'data/atis.test.csv'})\n","atis"]},{"cell_type":"code","execution_count":11,"id":"00c3304d","metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text', 'tag'],\n","    num_rows: 4274\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_data = atis['train']\n","val_data = atis['val']\n","test_data = atis['test']\n","\n","train_data.shuffle(seed=seed)"]},{"attachments":{},"cell_type":"markdown","id":"1ae9bc9c","metadata":{},"source":["We build tokenizers from the training data to tokenize both text and tag and convert them into word ids."]},{"cell_type":"code","execution_count":12,"id":"06f46dee","metadata":{},"outputs":[],"source":["MIN_FREQ = 3\n","unk_token = '[UNK]'\n","pad_token = '[PAD]'\n","bos_token = '<bos>'\n","\n","def train_tokenizers(dataset, min_freq):\n","    text_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n","    text_tokenizer.pre_tokenizer = WhitespaceSplit()\n","    text_tokenizer.normalizer = normalizers.Lowercase()\n","\n","    text_trainer = WordLevelTrainer(min_frequency=min_freq, special_tokens=[pad_token, unk_token,bos_token])\n","    text_tokenizer.train_from_iterator(dataset['text'], trainer=text_trainer)\n","    text_tokenizer.post_processor = TemplateProcessing(single=f\"{bos_token} $A\", special_tokens=[(bos_token, text_tokenizer.token_to_id(bos_token))])\n","\n","    tag_tokenizer = Tokenizer(WordLevel(unk_token=unk_token))\n","    tag_tokenizer.pre_tokenizer = WhitespaceSplit()\n","\n","    tag_trainer = WordLevelTrainer(special_tokens=[pad_token, unk_token, bos_token])\n","\n","    tag_tokenizer.train_from_iterator(dataset['tag'], trainer=tag_trainer)\n","\n","    tag_tokenizer.post_processor = TemplateProcessing(single=f\"{bos_token} $A\", special_tokens=[(bos_token, tag_tokenizer.token_to_id(bos_token))])\n","    return text_tokenizer, tag_tokenizer\n","\n","text_tokenizer, tag_tokenizer = train_tokenizers(train_data, MIN_FREQ)\n"]},{"attachments":{},"cell_type":"markdown","id":"aeb9f3c0","metadata":{},"source":["We use `datasets.Dataset.map` to convert text into word ids. As shown in lab 1-5, first we need to wrap `tokenizer` with the `transformers.PreTrainedTokenizerFast` class to be compatible with the `datasets` library."]},{"cell_type":"code","execution_count":13,"id":"35c65085","metadata":{},"outputs":[],"source":["hf_text_tokenizer = PreTrainedTokenizerFast(tokenizer_object=text_tokenizer, pad_token=pad_token, unk_token=unk_token, bos_token=bos_token)\n","\n","hf_tag_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tag_tokenizer, pad_token=pad_token, unk_token=unk_token, bos_token=bos_token)"]},{"cell_type":"code","execution_count":14,"id":"60d4e05e","metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"109f5afd72b948f38651be5aea928c15","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4274 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7bd5fd9212ed47a7923e38c04eda7838","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/572 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe8e256d45cb44a79133e28ad4fda0fa","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/586 [00:00<?, ?ex/s]"]},"metadata":{},"output_type":"display_data"}],"source":["def encode(example):\n","    example['input_ids'] = hf_text_tokenizer(example['text']).input_ids\n","    example['tag_ids'] = hf_tag_tokenizer(example['tag']).input_ids\n","    return example\n","\n","train_data = train_data.map(encode)\n","val_data = val_data.map(encode)\n","test_data = test_data.map(encode)"]},{"attachments":{},"cell_type":"markdown","id":"05a5a78a","metadata":{},"source":["We can get some sense of the datasets by looking at the size of the text and tag vocabularies."]},{"cell_type":"code","execution_count":15,"id":"9f5ebadd","metadata":{"colab":{},"colab_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of English vocabulary: 518\n","Number of tags: 104\n"]}],"source":["# Compute size of vocabulary\n","text_vocab = text_tokenizer.get_vocab()\n","tag_vocab = tag_tokenizer.get_vocab()\n","vocab_size = len(text_vocab)\n","num_tags = len(tag_vocab)\n","\n","print(f\"Size of English vocabulary: {vocab_size}\")\n","print(f\"Number of tags: {num_tags}\")"]},{"attachments":{},"cell_type":"markdown","id":"e3df9c48","metadata":{"colab_type":"text"},"source":["## Special tokens and tags\n","\n","You'll have already noticed the `BOS` and `EOS`, special tokens that the dataset developers used to indicate the beginning and end of the sentence; we'll leave them in the data.\n","\n","We've also prepended `<bos>` for both text and tag. `Tokenizers` will prepend these to the sequence of words and tags. This relieves us from estimating the initial distribution of tags and tokens in HMMs, since we always start with a token `<bos>` whose tag is also `<bos>`. We'll be able to refer to these tags as exemplified here:"]},{"cell_type":"code","execution_count":16,"id":"764263e1","metadata":{"colab":{},"colab_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Initial tag string: <bos>\n","Initial tag id:     2\n","\n"]}],"source":["print(f\"\"\"\n","Initial tag string: {bos_token}\n","Initial tag id:     {tag_vocab[bos_token]}\n","\"\"\")"]},{"attachments":{},"cell_type":"markdown","id":"1e666655","metadata":{"colab_type":"text"},"source":["Finally, since we will be providing the sentences in the training corpus in \"batches\", we willl force the sentences within a batch to be the same length by padding them with a special `[PAD]` token. Again, we can access that token as shown here:"]},{"cell_type":"code","execution_count":17,"id":"2da3862e","metadata":{"colab":{},"colab_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Pad tag string: [PAD]\n","Pad tag id:     0\n","\n"]}],"source":["print(f\"\"\"\n","Pad tag string: {pad_token}\n","Pad tag id:     {tag_vocab[pad_token]}\n","\"\"\")"]},{"attachments":{},"cell_type":"markdown","id":"f0999160","metadata":{},"source":["To load data in batched tensors, we use `torch.utils.data.DataLoader` for data splits, which enables us to iterate over the dataset under a given `BATCH_SIZE`. For the test set, we use a batch size of 1, to make the decoding implementation easier."]},{"cell_type":"code","execution_count":18,"id":"55fcc88b","metadata":{},"outputs":[],"source":["BATCH_SIZE = 32     # batch size for training and validation\n","\n","# Defines how to batch a list of examples together\n","def collate_fn(examples):\n","    batch = {}\n","    bsz = len(examples)\n","    input_ids, tag_ids = [], []\n","    for example in examples:\n","        input_ids.append(example['input_ids'])\n","        tag_ids.append(example['tag_ids'])\n","        \n","    max_length = max([len(word_ids) for word_ids in input_ids])\n","\n","    tag_batch = torch.zeros(bsz, max_length).long().fill_(tag_vocab[pad_token]).to(device)\n","    text_batch = torch.zeros(bsz, max_length).long().fill_(text_vocab[pad_token]).to(device)\n","    for b in range(bsz):\n","        text_batch[b][:len(input_ids[b])] = torch.LongTensor(input_ids[b]).to(device)\n","        tag_batch[b][:len(tag_ids[b])] = torch.LongTensor(tag_ids[b]).to(device)\n","    \n","    batch['tag_ids'] = tag_batch\n","    batch['input_ids'] = text_batch\n","    return batch\n","\n","def get_iterators(train_data, val_data, test_data):\n","    train_iter = torch.utils.data.DataLoader(train_data, \n","                                            batch_size=BATCH_SIZE, \n","                                            shuffle=True, \n","                                            collate_fn=collate_fn)\n","    val_iter = torch.utils.data.DataLoader(val_data, \n","                                        batch_size=BATCH_SIZE, \n","                                        shuffle=False, \n","                                        collate_fn=collate_fn)\n","    test_iter = torch.utils.data.DataLoader(test_data, \n","                                            batch_size=BATCH_SIZE, \n","                                            shuffle=False, \n","                                            collate_fn=collate_fn)\n","    return train_iter, val_iter, test_iter\n","\n","train_iter, val_iter, test_iter = get_iterators(train_data, val_data, test_data)"]},{"attachments":{},"cell_type":"markdown","id":"319ccb5c","metadata":{"colab_type":"text"},"source":["Now, we can iterate over the dataset. We used a non-trivial batch size to gain the benefit of training on multiple sentences at a shot. You'll need to be careful about the shapes of the various tensors that are being manipulated."]},{"attachments":{},"cell_type":"markdown","id":"937dd10a","metadata":{},"source":["Each batch will be a tensor of size `batch_size x max_length`. Let's examine a batch."]},{"cell_type":"code","execution_count":19,"id":"77ded5ff","metadata":{"colab":{},"colab_type":"code"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of batch text tensor: torch.Size([32, 21])\n","\n","First sentence in batch\n","tensor([  2,   3,  82, 154,   7,  31,  50,  36,  14,  19,  20,  29,   9, 121,\n","        431,   4,   0,   0,   0,   0,   0])\n","<bos> bos how many flights are there between san francisco and philadelphia on august eighteenth eos [PAD] [PAD] [PAD] [PAD] [PAD]\n","\n","First tags in batch\n","tensor([ 2,  3,  3,  3,  3,  3,  3,  3,  5,  8,  3,  4,  3, 13, 12,  3,  0,  0,\n","         0,  0,  0])\n","<bos> O O O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name O B-depart_date.month_name B-depart_date.day_number O [PAD] [PAD] [PAD] [PAD] [PAD]\n"]}],"source":["# Get the first batch\n","batch = next(iter(train_iter))\n","\n","# What's its shape? Should be batch_size x max_length.\n","print(f'Shape of batch text tensor: {batch[\"input_ids\"].shape}\\n')\n","\n","# Extract the first sentence in the batch, both text and tags\n","first_sentence = batch['input_ids'][0]\n","first_tags = batch['tag_ids'][0]\n","\n","# Print out the first sentence, as token ids and as text\n","print(\"First sentence in batch\")\n","print(f\"{first_sentence}\")\n","print(f\"{hf_text_tokenizer.decode(first_sentence)}\\n\")\n","\n","print(\"First tags in batch\")\n","print(f\"{first_tags}\")\n","print(f\"{hf_tag_tokenizer.decode(first_tags)}\")"]},{"attachments":{},"cell_type":"markdown","id":"5e5e876b","metadata":{"colab_type":"text"},"source":["The goal of this project is to predict the sequence of tags `batch['tag_ids']` given a sequence of words `batch['input_ids']`."]},{"attachments":{},"cell_type":"markdown","id":"7c792233","metadata":{},"source":["# Majority class labeling\n","\n","As usual, we can get a sense of the difficulty of the task by looking at a simple baseline, tagging every token with the majority tag. Here's a table of tag frequencies for the most frequent tags:"]},{"cell_type":"code","execution_count":20,"id":"2d824218","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["  0  [PAD]                           0\n","  1  [UNK]                           0\n","  2  <bos>                         4274\n","  3  O                             38967\n","  4  B-toloc.city_name             3751\n","  5  B-fromloc.city_name           3726\n","  6  I-toloc.city_name             1039\n","  7  B-depart_date.day_name        835\n","  8  I-fromloc.city_name           636\n","  9  B-airline_name                610\n"," 10  B-depart_time.period_of_day   555\n"," 11  I-airline_name                374\n"," 12  B-depart_date.day_number      351\n"," 13  B-depart_date.month_name      340\n"," 14  B-depart_time.time            321\n"," 15  B-round_trip                  311\n"," 16  I-round_trip                  303\n"," 17  B-depart_time.time_relative   290\n"," 18  B-cost_relative               281\n"," 19  B-flight_mod                  264\n"," 20  I-depart_time.time            258\n"," 21  B-stoploc.city_name           202\n"," 22  B-city_name                   191\n"," 23  B-arrive_time.time            182\n"," 24  B-class_type                  181\n"," 25  B-arrive_time.time_relative   162\n"," 26  I-class_type                  148\n"," 27  I-arrive_time.time            142\n"," 28  B-flight_stop                 141\n"," 29  B-airline_code                109\n"," 30  I-depart_date.day_number      105\n"," 31  I-fromloc.airport_name        103\n"," 32  B-toloc.state_name             84\n"," 33  B-toloc.state_code             81\n"," 34  B-arrive_date.day_name         78\n"," 35  B-fromloc.airport_name         75\n"," 36  B-depart_date.date_relative    72\n"," 37  B-flight_number                72\n"," 38  B-depart_date.today_relative   70\n"," 39  I-airport_name                 61\n"," 40  I-city_name                    53\n"," 41  B-arrive_time.period_of_day    51\n"," 42  B-fare_basis_code              51\n"," 43  B-flight_time                  51\n"," 44  B-fromloc.state_code           51\n"," 45  B-or                           49\n"," 46  B-aircraft_code                48\n"," 47  B-meal_description             48\n"," 48  B-meal                         47\n"," 49  I-cost_relative                45\n"," 50  I-stoploc.city_name            45\n"," 51  B-airport_name                 44\n"," 52  B-transport_type               43\n"," 53  B-fromloc.state_name           42\n"," 54  B-arrive_date.day_number       40\n"," 55  B-arrive_date.month_name       40\n"," 56  B-depart_time.period_mod       39\n"," 57  B-flight_days                  37\n"," 58  B-connect                      36\n"," 59  I-toloc.airport_name           35\n"," 60  B-fare_amount                  34\n"," 61  I-fare_amount                  33\n"," 62  B-economy                      32\n"," 63  B-toloc.airport_name           28\n"," 64  B-mod                          24\n"," 65  I-flight_time                  24\n"," 66  B-airport_code                 22\n"," 67  B-depart_date.year             20\n"," 68  B-toloc.airport_code           19\n"," 69  B-arrive_time.start_time       18\n"," 70  B-depart_time.end_time         18\n"," 71  B-depart_time.start_time       18\n"," 72  I-transport_type               18\n"," 73  B-arrive_time.end_time         17\n"," 74  I-arrive_time.end_time         16\n"," 75  B-fromloc.airport_code         14\n"," 76  B-restriction_code             14\n"," 77  I-depart_time.end_time         13\n"," 78  I-flight_mod                   12\n"," 79  I-flight_stop                  12\n"," 80  B-arrive_date.date_relative    10\n"," 81  I-toloc.state_name             10\n"," 82  I-restriction_code              9\n"," 83  B-return_date.date_relative     8\n"," 84  I-depart_time.start_time        8\n"," 85  I-economy                       8\n"," 86  B-state_code                    7\n"," 87  I-arrive_time.start_time        7\n"," 88  I-fromloc.state_name            7\n"," 89  B-state_name                    6\n"," 90  I-depart_date.today_relative    6\n"," 91  I-depart_time.period_of_day     5\n"," 92  B-period_of_day                 4\n"," 93  I-arrive_date.day_number        4\n"," 94  B-day_name                      3\n"," 95  B-meal_code                     3\n"," 96  B-stoploc.state_code            3\n"," 97  B-arrive_time.period_mod        2\n"," 98  B-toloc.country_name            2\n"," 99  I-arrive_time.time_relative     2\n","100  I-meal_code                     2\n","101  I-return_date.date_relative     2\n","102  B-return_date.day_number        1\n","103  B-return_date.month_name        1\n"]}],"source":["def count_tags(iterator):\n","  tag_counts = torch.zeros(len(tag_vocab), device=device)\n","\n","  for batch in iterator:\n","    tags = batch['tag_ids'].view(-1)\n","    tag_counts.scatter_add_(0, tags, torch.ones(tags.shape).to(device))\n","\n","  ## Alternative untensorized implementation for reference\n","  # for batch in iterator:                # for each batch\n","  #   for sent_id in range(len(batch)):   # ... each sentence in the batch\n","  #     for tag in batch.tag[:, sent_id]: # ... each tag in the sentence\n","  #       tag_counts[tag] += 1            # bump the tag count\n","\n","  # Ignore paddings\n","  tag_counts[tag_vocab[pad_token]] = 0\n","  return tag_counts\n","\n","tag_counts = count_tags(train_iter)\n","\n","for tag_id in range(len(tag_vocab)):\n","  print(f'{tag_id:3}  {hf_tag_tokenizer.decode(tag_id):30}{tag_counts[tag_id].item():3.0f}')"]},{"attachments":{},"cell_type":"markdown","id":"ecd426c0","metadata":{},"source":["It looks like the `'O'` (other) tag is, unsurprisingly, the most frequent tag (except for the padding tag). The proportion of tokens labeled with that tag (ignoring the padding tag) gives us a good baseline accuracy for this sequence labeling task. To verify that intuition, we can calculate the accuracy of the majority tag on the test set:"]},{"cell_type":"code","execution_count":21,"id":"e02a1095","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Baseline accuracy: 0.634\n"]}],"source":["tag_counts_test = count_tags(test_iter)\n","majority_baseline_accuracy = (\n","  tag_counts_test[tag_vocab['O']] \n","  / tag_counts_test.sum()\n",")\n","print(f'Baseline accuracy: {majority_baseline_accuracy:.3f}')"]},{"attachments":{},"cell_type":"markdown","id":"9d9849fe","metadata":{"colab_type":"text"},"source":["# HMM for sequence labeling\n","\n","Having established the baseline to beat, we turn to implementing an HMM model.\n","\n","## Notation\n","\n","First, let's start with some notation. We use $\\mathcal{V} = \\langle \\mathcal{V}_1, \\mathcal{V}_2, \\ldots \\mathcal{V}_V \\rangle$ to denote the vocabulary of word types and $Q = \\langle{Q_1, Q_2, \\ldots, Q_N} \\rangle$ to denote the possible tags, which is the state space of the HMM. Thus $V$ is the number of word types in the vocabulary and $N$ is the number of states (tags).\n","\n","We use $\\vect{w} = w_1 \\cdots w_T \\in \\mathcal{V}^T$ to denote the string of words at \"time steps\" $t$ (where $t$ varies from $1$ to $T$). Similarly, $\\vect{q} = q_1 \\cdots q_T \\in Q^T$ denotes the corresponding sequence of states (tags)."]},{"attachments":{},"cell_type":"markdown","id":"0141305b","metadata":{"colab_type":"text"},"source":["## Training an HMM by counting\n","\n","Recall that an HMM is defined via a transition matrix $A$, which stores the probability of moving from one state $Q_i$ to another $Q_j$, that is, \n","\n","$$A_{ij}=\\Prob(q_{t+1}=Q_j  \\given  q_t=Q_i)$$\n","\n","and an emission matrix $B$, which stores the probability of generating word $\\mathcal{V}_j$ given state $Q_i$, that is, \n","\n","$$B_{ij}= \\Prob(w_t=\\mathcal{V}_j  \\given q_t= Q_i)$$\n","\n","> As is typical in notating probabilities, we'll use abbreviations\n",">\n","\\begin{align}\n","\\Prob(q_{t+1} \\given  q_t) &\\equiv \\Prob(q_{t+1}=Q_j  \\given  q_t=Q_i) \\\\\n","\\Prob(w_t  \\given q_t) &\\equiv \\Prob(w_t=\\mathcal{V}_j  \\given q_t= Q_i)\n","\\end{align}\n",">\n","> where the $i$ and $j$ are clear from context.\n","\n","In our case, since the labels are observed in the training data, we can directly use counting to determine (maximum likelihood) estimates of $A$ and $B$."]},{"attachments":{},"cell_type":"markdown","id":"7b35855b","metadata":{"colab_type":"text"},"source":["### Goal 1(a): Find the transition matrix\n","\n","The matrix $A$ contains the transition probabilities: $A_{ij}$ is the probability of moving from state $Q_i$ to state $Q_j$ in the training data, so that $\\sum^{N}_{j = 1 } A_{ij} = 1$ for all $i$. \n","\n","We find these probabilities by counting the number of times state $Q_j$ appears right after state $Q_i$, as a proportion of all of the transitions from $Q_i$.\n","\n","$$\n","A_{ij} = \\frac{\\cnt{Q_i, Q_j} + \\delta}{\\sum_k \\left (\\cnt{Q_i, Q_k}+\\delta \\right)}\n","$$\n","\n","(In the above formula, we also used add-$\\delta$ smoothing.)\n","\n","Using the above definition, implement the method `train_A` in the `HMM` class below, which calculates and returns the $A$ matrix as a tensor of size $N \\times N$.\n","\n","> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal.\n","\n","> Remember that the training data is being delivered to you batched."]},{"attachments":{},"cell_type":"markdown","id":"1f0b1542","metadata":{"colab_type":"text"},"source":["### Goal 1(b): Find the emission matrix $B$\n","\n","Similar to the transition matrix, the emission matrix contains the emission probabilities such that $B_{ij}$ is probability of word $w_t=\\mathcal{V}_j$ conditioned on state $q_t=Q_i$.\n","\n","We can find this by counting as well.\n","$$\n","B_{ij} = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\sum_k \\left (\\cnt{Q_i, \\mathcal{V}_k} + \\delta \\right)}\n","       = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\cnt{Q_i} + \\delta V}\n","$$\n","\n","Using the above definitions, implement the `train_B` method in the `HMM` class below, which calculates and returns the $B$ matrix as a tensor of size $N \\times V$.\n","\n","> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal."]},{"attachments":{},"cell_type":"markdown","id":"03e9f994","metadata":{"colab_type":"text"},"source":["## Sequence labeling with a trained HMM\n","\n","Now that you're able to train an HMM by estimating the transition matrix $A$ and the emission matrix $B$, you can apply it to the task of labeling a sequence of words $\\vect{w} = w_1 \\cdots w_T$. Our goal is to find the most probable sequence of tags $\\vect{\\hat q} \\in Q^T$ given a sequence of words $\\vect{w} \\in \\mathcal{V}^T$.\n","\n","\\begin{align*}\n","\\vect{\\hat q} &= \\operatorname*{argmax}\\limits_{\\vect{q} \\in Q^T}(\\Prob(\\vect{q} \\given \\vect{w})) \\\\\n","& = \\operatorname*{argmax}_{\\vect{q} \\in Q^T}(\\Prob(\\vect{q},\\vect{w})) \\\\\n","& = \\operatorname*{argmax}_{\\vect{q} \\in Q^T}\\left(\\Pi^{T}_{t = 1} \\Prob(w_{t} \\given q_{t})\\Prob(q_{t} \\given q_{t-1})\\right)\n","\\end{align*}\n","\n","where $\\Prob(w_{t}=\\mathcal{V}_j \\given q_{t}=Q_i) = B_{ij}$, $\\Prob(q_{t}=Q_j \\given q_{t-1}=Q_{i})=A_{ij}$, and $q_0$ is the predefined initial tag `TAG.vocab.stoi[TAG.init_token]`."]},{"attachments":{},"cell_type":"markdown","id":"1017f3d2","metadata":{"colab_type":"text"},"source":["### Goal 1(c): Viterbi algorithm\n","\n","Implement the `predict` method, which should use the Viterbi algorithm to find the most likely sequence of tags for a sequence of `words`.\n","\n","> Warning: It may take up to 30 minutes to tag the entire test set depending on your implementation. (A fully tensorized implementation can be much faster though.) We highly recommend that you begin by experimenting with your code using a _very small subset_ of the dataset, say two or three sentences, ramping up from there.\n","\n","> Hint: Consider how to use vectorized computations where possible for speed."]},{"attachments":{},"cell_type":"markdown","id":"a83e4379","metadata":{},"source":["## Evaluation\n","\n","We've provided you with the `evaluate` function, which takes a dataset iterator and uses `predict` on each sentence in each batch, comparing against the gold tags, to determine the accuracy of the model on the test set."]},{"cell_type":"code","execution_count":38,"id":"c644b79d","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["class HMMTagger():\n","  def __init__ (self, hf_text_tokenizer, hf_tag_tokenizer):\n","    self.hf_text_tokenizer = hf_text_tokenizer\n","    self.hf_tag_tokenizer = hf_tag_tokenizer\n","\n","    self.V = len(self.hf_text_tokenizer)    # vocabulary size\n","    self.N = len(self.hf_tag_tokenizer)     # state space size\n","    \n","    self.initial_state_id = self.hf_tag_tokenizer.bos_token_id\n","    self.pad_state_id = self.hf_tag_tokenizer.pad_token_id\n","    self.pad_word_id = self.hf_text_tokenizer.pad_token_id\n","  \n","  def train_A(self, iterator, delta):\n","    \"\"\"Returns A for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n","    # Create A table\n","    A = torch.zeros(self.N, self.N, device=device)\n","\n","    #TODO: Add your solution from Goal 1(a) here.\n","    #      The returned value should be a tensor for the A matrix\n","    #      of size N x N.\n","    train_iter = iter(iterator)\n","    for batch in  tqdm(train_iter):\n","      tagList = batch['tag_ids']\n","      for i in range(len(tagList)):\n","        taggedSentence = tagList[i]\n","        for j in range(len(taggedSentence)-1):\n","          A[taggedSentence[j]][taggedSentence[j+1]] += 1 #counting the number of Q[j] -> Q[j+1]\n","          # TODO: maybe need to ignore padding also?\n","    denominator = torch.sum(A,1) #for each row, sums up the transitions\n","\n","    A += delta\n","    denominator += delta * self.N\n","    A /= denominator.unsqueeze(1)\n","    #for i in range(self.N):\n","    #  torch.div(A[i],denominator[i]) #divided each element of the row by the total transitions of the row\n","    return A\n","\n","  def train_B(self, iterator, delta):\n","    \"\"\"Returns B for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n","    # Create B\n","    B = torch.zeros(self.N, self.V, device=device)\n","    \n","    #TODO: Add your solution from Goal 1 (b) here.\n","    #      The returned value should be a tensor for the $B$ matrix\n","    #      of size N x V.\n","    \n","    train_iter = iter(iterator)\n","    for batch in  tqdm(train_iter):\n","      sentenceList = batch['input_ids']\n","      tagList = batch['tag_ids']\n","      for i in range(len(tagList)):\n","        taggedSentence = tagList[i]\n","        wordSentence = sentenceList[i]\n","        for j in range(len(wordSentence)):\n","          B[taggedSentence[j]][wordSentence[j]] += 1\n","\n","    denominator = torch.sum(B,1) #for each row, sums up the transitions\n","    B += delta\n","    denominator += delta * self.V\n","    B /= denominator.unsqueeze(1)\n","    #for i in range(self.N):\n","    #  torch.div(B[i],denominator[i]) #divided each element of the row by the total transitions of the row\n","    \n","    return B\n","\n","  def train_all(self, iterator, delta=0.01):\n","    \"\"\"Stores A and B (actually, their logs) for training dataset `iterator`.\"\"\"\n","    self.log_A = self.train_A(iterator, delta).log()\n","    self.log_B = self.train_B(iterator, delta).log()\n","    \n","  def predict(self, words):\n","    \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\n","    Arguments:\n","      words: a tensor of size (seq_len,)\n","    Returns:\n","      a list of tag ids\n","    \"\"\"\n","    #TODO: Add your solution from Goal 1 (c) here.\n","    #      The returned value should be a list of tag ids.\n","\n","    if len(words) == 0: #check if the length of the words is zero\n","      return []\n","    \n","    viterbiTable = torch.zeros(self.N, len(words))\n","    BPtable = torch.zeros(self.N, len(words))\n","\n","    viterbiTable[self.initial_state_id][0] = 1\n","    viterbiTable = torch.log(viterbiTable)\n","\n","    # viterbiTable[0] = self.log_A[self.initial_state_id] + self.log_B[:, words[0]]\n","    for i in range(1, len(words)):\n","      possibleProb = self.log_A + viterbiTable[:,i-1].view(-1,1)\n","      maxProb, maxID = torch.max(possibleProb, dim=0)\n","      viterbiTable[:,i] =  maxProb + self.log_B[:,words[i]]\n","      BPtable[:,i] =  maxID\n","\n","    bestpath = []\n","    # bestpath[len(words) - 1] = torch.argmax(viterbiTable[len(words)-1])\n","    bestpath.insert(0,torch.argmax(viterbiTable[:,-1]))\n","    for i in range(len(words)-1,0, -1):\n","      prev = int(bestpath[0].item())\n","      val = BPtable[prev][i]\n","      bestpath.insert(0,val)\n","\n","\n","    return bestpath\n","    \n","  def evaluate(self, iterator):\n","    \"\"\"Returns the model's token accuracy on a given dataset `iterator`.\"\"\"\n","    correct = 0\n","    total = 0\n","    for batch in tqdm(iterator, leave=False):\n","      for sent_id in range(len(batch['input_ids'])):\n","        words = batch['input_ids'][sent_id]\n","        words = words[words.ne(self.pad_word_id)] # remove paddings\n","        tags_gold = batch['tag_ids'][sent_id]\n","        tags_pred = self.predict(words)\n","        for tag_gold, tag_pred in zip(tags_gold, tags_pred):\n","          if tag_gold == self.pad_state_id:  # stop once we hit padding\n","            break\n","          else:\n","            total += 1\n","            if tag_pred == tag_gold:\n","              correct += 1\n","    return correct/total"]},{"attachments":{},"cell_type":"markdown","id":"383b4512","metadata":{"colab_type":"text"},"source":["Putting everything together, you should now be able to train and evaluate the HMM. A correct implementation can be expected to reach above **90% test set accuracy** after running the following cell."]},{"cell_type":"code","execution_count":40,"id":"96c57711","metadata":{"colab":{},"colab_type":"code"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39237b829ec64640af84aecfe621b741","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/134 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e86eb0c285c411f8661f58501fe2c00","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/134 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"035b401979eb43ee83f6836d12559797","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/134 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62cb010a144f4920a99fdf54fbe721f9","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/19 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Training accuracy: 0.915\n","Test accuracy:     0.906\n"]}],"source":["# Instantiate and train classifier\n","hmm_tagger = HMMTagger(hf_text_tokenizer, hf_tag_tokenizer)\n","hmm_tagger.train_all(train_iter)\n","\n","# Evaluate model performance\n","print(f'Training accuracy: {hmm_tagger.evaluate(train_iter):.3f}\\n'\n","      f'Test accuracy:     {hmm_tagger.evaluate(test_iter):.3f}')"]},{"attachments":{},"cell_type":"markdown","id":"e41cf848","metadata":{"colab_type":"text"},"source":["# RNN for Sequence Labeling\n","\n","HMMs work quite well for this sequence labeling task. Now let's take an alternative (and more trendy) approach: RNN/LSTM-based sequence labeling. Similar to the HMM part of this project, you will also need to train a model on the training data, and then use the trained model to decode and evaluate some testing data.\n","\n","<img src=\"https://github.com/nlp-course/data/raw/master/Resources/rnn-unfolded-figure.png\" width=600 align=right />\n","\n","After unfolding an RNN, the cell at time $t$ generates the observed output $\\vect{y}_t$ based on the input $\\vect{x}_t$ and the hidden state of the previous cell $\\vect{h}_{t-1}$, according to the following equations.\n","\n","\\begin{align*}\n","\\vect{h}_t &=  \\sigma(\\vect{U} \\vect{x}_t + \\vect{V} \\vect{h}_{t-1}) \\\\\n","\\vect{\\hat y}_t &= \\softmax(\\vect{W} \\vect{h}_t)\n","\\end{align*}\n","\n","The parameters here are the elements of the matrices $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$. Similar to the last project segment, we will perform the forward computation, calculate the loss, and then perform the backward computation to compute the gradients with respect to these model parameters. Finally, we will adjust the parameters opposite the direction of the gradients to minimize the loss, repeating until convergence.\n","\n","You've seen these kinds of neural network models before, for language modeling in lab 2-3 and sequence labeling in lab 2-5. The code there should be very helpful in implementing an `RNNTagger` class below. Consequently, we've provided very little guidance on the implementation. We do recommend you follow the steps below however."]},{"attachments":{},"cell_type":"markdown","id":"610d0d6c","metadata":{"colab_type":"text"},"source":["## Goal 2(a): RNN training\n","\n","Implement the forward pass of the RNN tagger and the loss function. A reasonable way to proceed is to implement the following methods:\n","\n","1. `forward(self, text_batch)`: Performs the RNN forward computation over a whole `text_batch` (`batch.text` in the above data loading example). The `text_batch` will be of shape `max_length x batch_size`. You might run it through the following layers: an embedding layer, which maps each token index to an embedding of size `embedding_size` (so that the size of the mapped batch becomes `max_length x batch_size x embedding_size`); then an RNN, which maps each token embedding to a vector of `hidden_size` (the size of all outputs is `max_length x batch_size x hidden_size`); then a linear layer, which maps each RNN output element to a vector of size $N$ (which is commonly referred to as \"logits\", recall that $N=|Q|$, the size of the tag set).\n","\n","This function is expected to return `logits`, which provides a logit for each tag of each word of each sentence in the batch (structured as a tensor of size `max_length x batch_size x N`). \n","\n","> You might find the following functions useful: \n",">\n","> * [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n","> * [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n","> * [`nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n","\n","2. `compute_loss(self, logits, tags)`: Computes the loss for a batch by comparing `logits` of a batch returned by `forward` to `tags`, which stores the true tag ids for the batch. Thus `logits` is a tensor of size `max_length x batch_size x N`, and `tags` is a tensor of size `max_length x batch_size`. Note that the criterion functions in `torch` expect outputs of a certain shape, so you might need to perform some shape conversions.\n","\n","> You might find [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) from the last project segment useful. Note that if you use `nn.CrossEntropyLoss` then you should not use a softmax layer at the end since that's already absorbed into the loss function. Alternatively, you can use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) as the final sublayer in the forward pass, but then you need to use [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), which does not contain its own softmax. We recommend the former, since working in log space is usually more numerically stable.\n","\n","> Be careful about the shapes/dimensions of tensors. You might find [`torch.Tensor.view`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) useful for reshaping tensors.\n","\n","3. `train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001)`: Trains the model on training data generated by the iterator `train_iter` and validation data `val_iter`.The `epochs` and `learning_rate` variables are the number of epochs (number of times to run through the training data) to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine which model was the best one as the epocks go by. Notice that our code below assumes that during training the best model is stored so that `rnn_tagger.load_state_dict(rnn_tagger.best_model)` restores the parameters of the best model."]},{"attachments":{},"cell_type":"markdown","id":"155f4fca","metadata":{"colab_type":"text"},"source":["## Goal 2(b) RNN decoding\n","\n","Implement a method to predict the tag sequence associated with a sequence of words:\n","\n","1. `predict(self, text_batch)`: Returns the batched predicted tag sequences associated with a batch of sentences.\n","2. `def evaluate(self, iterator)`: Returns the accuracy of the trained tagger on a dataset provided by `iterator`."]},{"cell_type":"code","execution_count":null,"id":"9ae2d180","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["class RNNTagger(nn.Module): \n","  ..."]},{"attachments":{},"cell_type":"markdown","id":"5717df76","metadata":{"colab_type":"text"},"source":["Now train your tagger on the training and validation set.\n","Run the cell below to train an RNN, and evaluate it. A proper implementation should reach about **95%+ accuracy**."]},{"cell_type":"code","execution_count":null,"id":"243ff348","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["# Instantiate and train classifier\n","rnn_tagger = RNNTagger(hf_text_tokenizer, hf_tag_tokenizer, embedding_size=36, hidden_size=36).to(device)\n","rnn_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n","rnn_tagger.load_state_dict(rnn_tagger.best_model)\n","\n","# Evaluate model performance\n","print(f'Training accuracy: {rnn_tagger.evaluate(train_iter):.3f}\\n'\n","      f'Test accuracy:     {rnn_tagger.evaluate(test_iter):.3f}')"]},{"attachments":{},"cell_type":"markdown","id":"daeddcb2","metadata":{"colab_type":"text"},"source":["# LSTM for slot filling\n","\n","Did your RNN perform better than HMM? How much better was it? Was that expected? \n","\n","RNNs tend to exhibit the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). To remedy this, the Long-Short Term Memory (LSTM) model was introduced. In PyTorch, we can simply use [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n","\n","In this section, you'll implement an LSTM model for slot filling. If you've got the RNN model well implemented, this should be extremely straightforward. Just copy and paste your solution, change the call to `nn.RNN` to a call to `nn.LSTM`, and make any other minor adjustments that are necessary. In particular, LSTMs have _two_ recurrent parts, `h` and `c`. You'll thus need to initialize both of these when performing forward computations."]},{"cell_type":"code","execution_count":null,"id":"275b2957","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["class LSTMTagger(nn.Module):\n","  ..."]},{"attachments":{},"cell_type":"markdown","id":"8d12455e","metadata":{"colab_type":"text"},"source":["Run the cell below to train an LSTM, and evaluate it. A proper implementation should reach about **94%+ accuracy**."]},{"cell_type":"code","execution_count":null,"id":"02efed50","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["# Instantiate and train classifier\n","lstm_tagger = LSTMTagger(hf_text_tokenizer, hf_tag_tokenizer, embedding_size=36, hidden_size=36).to(device)\n","lstm_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n","lstm_tagger.load_state_dict(lstm_tagger.best_model)\n","\n","# Evaluate model performance\n","print(f'Training accuracy: {lstm_tagger.evaluate(train_iter):.3f}\\n'\n","      f'Test accuracy:     {lstm_tagger.evaluate(test_iter):.3f}')"]},{"attachments":{},"cell_type":"markdown","id":"b4dc6b43","metadata":{"colab_type":"text"},"source":["# Goal 4: Compare HMM to RNN/LSTM with different amounts of training data\n","\n","Vary the amount of training data and compare the performance of HMM to RNN or LSTM (Since RNN is similar to LSTM, picking one of them is enough.) Discuss the pros and cons of HMM and RNN/LSTM based on your experiments.\n","\n","> This part is more open-ended. We're looking for thoughtful experiments and analysis of the results, not any particular result or conclusion.\n","\n","The code below shows how to subsample the training set with downsample ratio `ratio`. To speedup evaluation we only use 50 test samples."]},{"cell_type":"code","execution_count":null,"id":"d7aaa21e","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["ratio = 0.1\n","test_size = 50\n","\n","# Set random seeds to make sure subsampling is the same for HMM and RNN\n","random.seed(seed)\n","torch.manual_seed(seed)\n","\n","atis = load_dataset('csv', data_files={'train':'data/atis.train.csv', \\\n","                                       'val': 'data/atis.dev.csv', \\\n","                                       'test': 'data/atis.test.csv'})\n","train_data = atis['train']\n","test_data = atis['test']\n","\n","# Subsample\n","train_data = train_data.shuffle(seed=seed)\n","train_data = train_data.select(list(range(len(train_data)))[:int(math.floor(len(train_data)*ratio))])\n","test_data = test_data.shuffle(seed=seed)\n","test_data = test_data.select(list(range(len(test_data)))[:test_size])\n","\n","# Rebuild vocabulary\n","text_tokenizer, tag_tokenizer = train_tokenizers(train_data, MIN_FREQ)\n","\n","# Encode data\n","hf_text_tokenizer = PreTrainedTokenizerFast(tokenizer_object=text_tokenizer, pad_token=pad_token)\n","\n","hf_tag_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tag_tokenizer, pad_token=pad_token)\n","\n","def encode(example):\n","    example['input_ids'] = hf_text_tokenizer(example['text']).input_ids\n","    example['tag_ids'] = hf_tag_tokenizer(example['tag']).input_ids\n","    return example\n","\n","train_data = train_data.map(encode)\n","test_data = test_data.map(encode)\n","\n","# Create iterators\n","train_iter, val_iter, test_iter = get_iterators(train_data, val_data, test_data)"]},{"cell_type":"code","execution_count":null,"id":"1df2f3fe","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["..."]},{"cell_type":"code","execution_count":null,"id":"1485c352","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["..."]},{"cell_type":"code","execution_count":null,"id":"8cfa9b14","metadata":{"colab":{},"colab_type":"code"},"outputs":[],"source":["..."]},{"attachments":{},"cell_type":"markdown","id":"c389593c","metadata":{},"source":["_Type your answer here, replacing this text._"]},{"attachments":{},"cell_type":"markdown","id":"c18f7787","metadata":{"deletable":false,"editable":false},"source":["<!-- BEGIN QUESTION -->\n","\n","# Debrief\n","\n","**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n","\n","* Was the project segment clear or unclear? Which portions?\n","* Were the readings appropriate background for the project segment? \n","* Are there additions or changes you think would make the project segment better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"attachments":{},"cell_type":"markdown","id":"f3705ac5","metadata":{},"source":["_Type your answer here, replacing this text._"]},{"attachments":{},"cell_type":"markdown","id":"3151470e","metadata":{},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# Instructions for submission of the project segment\n","\n","This project segment should be submitted to Gradescope at <https://rebrand.ly/project2-submit-code> and <https://rebrand.ly/project2-submit-pdf>, which will be made available some time before the due date.\n","\n","Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <https://rebrand.ly/project2-submit-code>.\n","\n","We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <https://rebrand.ly/project2-submit-pdf>."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"include_colab_link":true,"name":"project2_sequence.ipynb","provenance":[]},"kernelspec":{"display_name":"otter-latest","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"title":"CS236299 Project Segment 2: Sequence labeling â€“ The slot filling task","vscode":{"interpreter":{"hash":"4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"}}},"nbformat":4,"nbformat_minor":5}
