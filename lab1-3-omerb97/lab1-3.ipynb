{"cells":[{"cell_type":"code","execution_count":1,"id":"b64b1325","metadata":{"deletable":false,"editable":false,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"]},{"name":"stderr","output_type":"stream","text":["\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"]}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","     \n","       Prints the result to stdout and returns the exit status. \n","       Provides a printed warning on non-zero exit status unless `warn` \n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2023-spring/lab1-3.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"]},{"cell_type":"code","execution_count":2,"id":"ea1ee6a1","metadata":{"deletable":false,"editable":false},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"cell_type":"raw","id":"db8b78c1","metadata":{"jupyter":{"source_hidden":true}},"source":["%%latex\n","\\newcommand{\\vect}[1]{\\mathbf{#1}}\n","\\newcommand{\\cnt}[1]{\\sharp(#1)}\n","\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\newcommand{\\softmax}{\\operatorname{softmax}}\n","\\newcommand{\\Prob}{\\Pr}\n","\\newcommand{\\given}{\\,|\\,}"]},{"cell_type":"markdown","id":"c0eedd05","metadata":{},"source":["$$\n","\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n","\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n","\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n","\\renewcommand{\\softmax}{\\operatorname{softmax}}\n","\\renewcommand{\\Prob}{\\Pr}\n","\\renewcommand{\\given}{\\,|\\,}\n","$$"]},{"cell_type":"markdown","id":"0dd4661e","metadata":{"tags":["remove_for_latex"]},"source":["# Course 236299\n","## Lab 1-3 – Naive Bayes classification"]},{"cell_type":"markdown","id":"0f675eda","metadata":{},"source":["In this lab, you'll apply the naive Bayes method to the _Federalist_ papers' authorship attribution problem.\n","\n","After this lab, you should be able to\n","\n","* Derive the basic equations for the naive Bayes classification method;\n","* Estimate the parameters for the naive Bayes model;\n","* Determine where use of the \"log trick\" is indicated, and apply it."]},{"cell_type":"markdown","id":"d1a79f31","metadata":{},"source":["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n","\n","* [`math.log2`](https://docs.python.org/3.8/library/math.html#math.log2)"]},{"cell_type":"markdown","id":"70707e2a","metadata":{},"source":["# Preparation – Loading packages and data"]},{"cell_type":"code","execution_count":3,"id":"8721ab97","metadata":{"jupyter":{"source_hidden":true}},"outputs":[],"source":["%matplotlib inline\n","import json\n","import math\n","import matplotlib\n","import matplotlib.pyplot as plt\n","matplotlib.style.use('tableau-colorblind10')\n","import torch\n","import wget\n","\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":4,"id":"3d16acc9","metadata":{},"outputs":[],"source":["# Download and read the Federalist data from the json file\n","os.makedirs('data', exist_ok=True)\n","wget.download('https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json', out='data/')\n","with open('data/federalist_data.json', 'r') as fin:\n","    dataset = json.load(fin)"]},{"cell_type":"code","execution_count":5,"id":"d9e2687f","metadata":{},"outputs":[],"source":["# As before, we extract the papers by either of Madison and Hamilton\n","# to serve as training data.\n","training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n","                       dataset))"]},{"cell_type":"markdown","id":"c66cd3e1","metadata":{},"source":["# The Naive Bayes method reviewed\n","A quick review of the Naive Bayes (NB) method for text classification: In classification tasks, we're given a representation of some text as a vector $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ of feature values, and we'd like to determine which of a set of classes $\\{ y_1, y_2, \\ldots, y_k \\}$ the text should be classified as. \n","\n","> In the case at hand, the Federalist Papers, for a given document, we'll take $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ to be the sequence of words in the document, so each $x_i$ corresponds to a single word token.\n","\n","We might naturally think to choose that class that has the highest probability of being correct, that is, the class $y_i$ that maximizes $Pr(y_i \\mid \\mathbf{x})$.\n","\n","By Bayes rule (this is the \"Bayes\" part in the name \"Naive Bayes\"), \n","\n","\\begin{align*}\n","\\argmax{i} \\Prob(y_i \\given \\vect{x}) \n","&= \\argmax{i} \\frac{\\Prob(\\vect{x} \\given y_i) \\cdot \\Prob(y_i)}{\\Prob(\\vect{x})} \\\\\n","&= \\argmax{i} \\Prob(\\vect{x} \\given y_i) \\cdot \\Prob(y_i)\n","\\end{align*}"]},{"cell_type":"markdown","id":"ad0caa2c","metadata":{"deletable":false,"editable":false},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question**: Why can we drop the denominator in the last step of this derivation?\n","<!--\n","BEGIN QUESTION\n","name: open_response_denominator\n","manual: true\n","-->"]},{"attachments":{},"cell_type":"markdown","id":"dfdb74af","metadata":{},"source":["Because the denominator doesn't change with every class and we are always asking about the most likely class.\n","It is also always positive and therefore doesn't change the sign of the equation. Therefore the original argmax equation is not affected. "]},{"attachments":{},"cell_type":"markdown","id":"c5633c6b","metadata":{},"source":["<!-- END QUESTION -->\n","\n","\n","\n","We use the following terminology: $\\Prob(y_i)$ is the _prior probability_. $\\Prob(\\vect{x} \\given y_i)$ is the _likelihood_. \n","$\\Prob(y_i \\given \\vect{x})$ is the _posterior probability_.\n","\n","By the chain rule, \n","\n","\\begin{align*}\n","\\Prob(\\vect{x} \\given y_i) &= \\Prob(x_1, \\ldots, x_m \\given y_i) \\\\\n","&= \\Prob(x_1 \\given y_i) \\cdot \\Prob(x_2, \\ldots, x_m \\given x_1, y_i) \\\\\n","&= \\Prob(x_1 \\given y_i) \\cdot \\Prob(x_2 \\given x_1, y_i) \\cdot \\Prob(x_3, \\ldots,\n","x_m \\given x_1, x_2, y_i) \\\\\n","\\cdots &= \\prod_{j=1}^m \\Prob(x_j \\given x_1, \\ldots, x_{j-1}, y_i)\n","\\end{align*}\n","\n","We further assume that each feature $x_j$ is independent of all the others given the class. (That's the \"naive\" part.) So \n","\n","$$\n","\\Prob(x_j \\given x_1, \\ldots, x_{j-1}, y_i) \\approx \\Prob(x_j \\given y_i)\n","$$\n","\n","Using this approximation, we'll calculate instead the class as per the following maximization:\n","\n","$$\n","\\argmax{i} \\Prob(y_i \\given \\vect{x}) \\approx \\argmax{i} \\Prob(y_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given y_i)\n","$$\n","\n","> This independence assumption, in the text case, amounts to ignoring the order and even the cooccurence of words in a document, a quite aggressive and unrealistic independence assumption indeed.\n","\n","All we need, then, for the Naive Bayes classification method is values for $\\Prob(y_i)$ and $\\Prob(x_j \\given y_i)$ for each feature $x_j$ and each class $y_i$. These constitute the parameters of the model, which we will learn from a training dataset."]},{"attachments":{},"cell_type":"markdown","id":"3f1c94d5","metadata":{},"source":["# Naive Bayes for the Federalist papers\n","\n","In applying Naive Bayes to an example in the Federalist dataset, we'll take the $x_j$ to be the _tokens in the example_. To make the calculations easier, in this lab, we won't use _all_ of the tokens, just the tokens of the four word types we've been attending to, but in an actual application of NB, we'd use (essentially) all of the word types. As a reminder,"]},{"cell_type":"code","execution_count":6,"id":"44147732","metadata":{},"outputs":[],"source":["keywords = ['on', 'upon', 'there', 'whilst']"]},{"cell_type":"markdown","id":"ae458498","metadata":{},"source":["and the two class labels are"]},{"cell_type":"code","execution_count":7,"id":"16856cdc","metadata":{},"outputs":[],"source":["classes = ['Hamilton', 'Madison']"]},{"cell_type":"markdown","id":"5d8c1df9","metadata":{},"source":["> To clarify, we still treat the input as a bag-of-words representation, but the vocabulary is now limited to four word types."]},{"cell_type":"markdown","id":"0a989bce","metadata":{},"source":["### Estimating the prior probabilities"]},{"cell_type":"markdown","id":"a40d822d","metadata":{"deletable":false,"editable":false},"source":["Let's start with the prior probabilities $\\Prob(y_i)$. In our case, there are only two class labels, for Hamilton and Madison. We estimate the probability of a class $y_i$ by simply counting the proportion of examples that are labeled with that class. (This estimate is the *sample probability*, which is also referred to as the *maximum likelihood estimate* for reasons we'll skip for the moment.) That is, we estimate \n","\n","$$ \\Prob(y_i) \\approx \\frac{\\cnt{y_i}}{N} $$\n","\n","where $N$ is the number of training examples, and $\\cnt{y_i}$ is the number of training examples of class $y_i$.\n","\n","In the cell below, write code to count how many of the training examples are labeled with Hamilton and how many are labeled with Madison. Use these to provide estimates of the Hamilton and Madison prior probabilities.\n","<!--\n","BEGIN QUESTION\n","name: priors\n","-->"]},{"cell_type":"code","execution_count":8,"id":"dd3d66e5","metadata":{},"outputs":[],"source":["#TODO - Calculate the prior probabilities for Madison and Hamilton as floats.\n","prior_madison =len(list(filter(lambda ex: ex['authors'] in ['Madison'],\n","                       dataset)))/len(training)\n","prior_hamilton = len(list(filter(lambda ex: ex['authors'] in ['Hamilton'],\n","                       dataset)))/len(training)"]},{"cell_type":"code","execution_count":9,"id":"62fef384","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"priors\")"]},{"cell_type":"code","execution_count":10,"id":"49fc8a6a","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Madison  prior: 0.2273\n","Hamilton prior: 0.7727\n"]}],"source":["print(f\"Madison  prior: {prior_madison:.4f}\\n\"\n","      f\"Hamilton prior: {prior_hamilton:.4f}\")"]},{"cell_type":"markdown","id":"a8a485b4","metadata":{"deletable":false,"editable":false},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** What do these probabilities tell us about how we might predict the class of a _Federalist_ document _prior_ to looking at the actual content of the document? (That's why these probabilities are called \"priors\".)\n","<!--\n","BEGIN QUESTION\n","name: open_response_priors\n","manual: true\n","-->"]},{"attachments":{},"cell_type":"markdown","id":"c06d9cc1","metadata":{},"source":["Without any other evidence we would assume Hamilton wrote the unknown texts since he wrote most of the known texts. "]},{"cell_type":"markdown","id":"b9921350","metadata":{},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## Estimating the likelihood probabilities"]},{"cell_type":"markdown","id":"ef87208b","metadata":{},"source":["Now for the likelihood probabilities, the conditional probability of a word given a class. For each likelihood $\\Prob(x_j \\given y_i)$, we need to estimate a value. We'll do so by simply counting the number of training examples with feature value $x_j$ that are labeled $y_i$ (notated as $\\cnt{x_j, y_i}$) as a proportion of the overall number of words labeled as $y_i$, that is,\n","\n","$$ \\Prob(x_j \\given y_i) \\approx \\frac{\\cnt{x_j, y_i}}{\\sum_k \\cnt{x_k, y_i}} $$\n","\n","Again, for the text case, each token counts as an instance of the corresponding word type in a training example. Note that $\\sum_k \\cnt{x_k, y_i}$ is not the same as $\\cnt{y_i}$.\n"," \n","We've provided a small table that shows, for each label (author) and each of the four word types of interest, how many tokens of the type occurred in training examples with that label."]},{"cell_type":"code","execution_count":11,"id":"9eb7c682","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                on    upon   there  whilst\n","Hamilton       390     377     369       1\n","Madison        308       7      32      12\n"]}],"source":["def counts(dataset, label, index):\n","    \"\"\"Returns the total count for `index` for examples with the \n","       given `label`\"\"\"\n","    return sum([example['counts'][index] \n","                for example in dataset \n","                if example['authors'] == label])\n","\n","# print a table header\n","print(f\"{'':10}\", end=\"\")\n","for i in range(4):\n","    print(f\"{keywords[i]:>8}\", end=\"\")\n","print()\n","# print table entries for each label\n","for label in classes:\n","    print(f\"{label:10}\", end=\"\")\n","    for i in range(4):\n","        print(f\"{counts(training, label, i):8}\", end=\"\")\n","    print()"]},{"cell_type":"markdown","id":"f8bb316a","metadata":{"deletable":false,"editable":false},"source":["Given the counts in this table, what would an estimate be for the probability that a given word would be \"whilst\" given that the document was authored by Madison, that is, $\\Prob(\\mathrm{whilst} \\given \\mathrm{Madison})$?\n","<!--\n","BEGIN QUESTION\n","name: prob_whilst_madison\n","-->"]},{"cell_type":"code","execution_count":12,"id":"34298195","metadata":{},"outputs":[],"source":["#TODO - Define this variable to be the specified probability.\n","prob_whilst_madison = 12/(308+7+32+12)"]},{"cell_type":"code","execution_count":13,"id":"7b43049e","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"prob_whilst_madison\")"]},{"cell_type":"markdown","id":"e0699b20","metadata":{"deletable":false,"editable":false},"source":["What about the probability $\\Prob(\\mathrm{on} \\given \\mathrm{Hamilton})$?\n","<!--\n","BEGIN QUESTION\n","name: prob_on_hamilton\n","-->"]},{"cell_type":"code","execution_count":14,"id":"038ef343","metadata":{},"outputs":[],"source":["#TODO - Define this variable to be the specified probability.\n","prob_on_hamilton = 390/(390+377+369+1)"]},{"cell_type":"code","execution_count":15,"id":"602c8879","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"prob_on_hamilton\")"]},{"cell_type":"markdown","id":"04f9f121","metadata":{"deletable":false,"editable":false},"source":["Consider a sample text \n","\n","> **whilst** depending neither **on** the American government nor **on** the British\n","\n","What would the Naive Bayes method estimate be for the likelihood of this sentence if it was by Hamilton? By Madison? (You should of course ignore all the tokens in our little sample text except for tokens of the four keyword types. (We've boldfaced their occurrences.) With a full-blown NB analysis, we'd be using *all* of the words in the text.)\n","<!--\n","BEGIN QUESTION\n","name: likelihoods\n","-->"]},{"cell_type":"code","execution_count":16,"id":"b61e67ca","metadata":{},"outputs":[],"source":["#TODO - Define the variables to be the corresponding likelihood probabilities.\n","likelihood_hamilton = (prob_on_hamilton**2) * (1/(390+377+369+1))\n","likelihood_madison = prob_whilst_madison * ((308/(308+7+32+12))**2)"]},{"cell_type":"code","execution_count":17,"id":"a1de5c74","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"likelihoods\")"]},{"cell_type":"code","execution_count":18,"id":"2ac7534d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Madison  likelihood: 0.024604\n","Hamilton likelihood: 0.000103\n"]}],"source":["print(f\"Madison  likelihood: {likelihood_madison:4f}\\n\"\n","      f\"Hamilton likelihood: {likelihood_hamilton:4f}\")"]},{"cell_type":"markdown","id":"f8f9e5fa","metadata":{},"source":["## Posterior probabilities"]},{"cell_type":"markdown","id":"a0bfdb84","metadata":{"deletable":false,"editable":false},"source":["We're almost there. We simply need to combine the prior probabilities and the likelihood probabilities for each class to form the posterior, and select the largest one. As a reminder, we don't actually calculate the posterior _probability_ because we aren't dividing through by $\\Prob(\\vect{x})$. Instead, we get something like a posterior _score_.\n","\n","Calculate the posteriors for the two classes, and then specify which class – Hamilton or Madison – the NB method would predict for the sample text.\n","<!--\n","BEGIN QUESTION\n","name: posteriors\n","-->"]},{"cell_type":"code","execution_count":19,"id":"362846bc","metadata":{},"outputs":[],"source":["#TODO - Define the variables to be the corresponding posterior probabilities, \n","#       and the classification of the sample phrase.\n","posterior_madison = likelihood_madison * prior_madison\n","posterior_hamilton = likelihood_hamilton * prior_hamilton\n","sample_classification = \"Hamilton\" if posterior_hamilton > posterior_madison else \"Madison\""]},{"cell_type":"code","execution_count":20,"id":"c7c93d88","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"posteriors\")"]},{"cell_type":"code","execution_count":21,"id":"e71b89e0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Madison  posterior: 0.005592\n","Hamilton posterior: 0.000080\n","Sample classification: Madison\n"]}],"source":["print(f\"Madison  posterior: {posterior_madison:4f}\\n\"\n","      f\"Hamilton posterior: {posterior_hamilton:4f}\\n\"\n","      f\"Sample classification: {sample_classification}\")"]},{"cell_type":"markdown","id":"46caab1d","metadata":{"deletable":false,"editable":false},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** Is the NB-predicted classification the same as or different from the classification based on the priors? Why?\n","<!--\n","BEGIN QUESTION\n","name: open_response_nb_v_priors\n","manual: true\n","-->"]},{"attachments":{},"cell_type":"markdown","id":"e64c5f0f","metadata":{},"source":["Its diffrent. This is because we saw that Madison is much more likely to use whilst in his documents than Hamilton. "]},{"cell_type":"markdown","id":"be06ebad","metadata":{},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## A practical issue\n","\n","The computations of what we've been calling the posterior scores\n","$$\\Prob(y_i \\given \\vect{x}) \\approx \\Prob(y_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given y_i)$$\n","involve the multiplication of many extremely small numbers. This is a recipe for [_arithmetic underflow_](https://en.wikipedia.org/wiki/Arithmetic_underflow), leading to garbage outputs."]},{"cell_type":"code","execution_count":22,"id":"174574d6","metadata":{},"outputs":[{"data":{"text/plain":["0.0"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["# An example for an arithmetic unferflow\n","2**(-1076)"]},{"cell_type":"markdown","id":"97f465fe","metadata":{},"source":["Instead, rather than maximizing the posterior, we can maximize its logarithm. Since the logarithm function is monotonic (see the next cell for a figure), whichever  𝑖  maximizes the posterior maximizes its log as well."]},{"cell_type":"code","execution_count":23,"id":"2994c2b3","metadata":{"jupyter":{"source_hidden":true}},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["def log_plot():\n","    x = torch.linspace(1e-10, 1, 100)\n","    fig, ax = plt.subplots()\n","    ax.plot(x, torch.log2(x), label = \"log_2\")\n","    plt.title(\"Monotonicity of logarithm\")\n","    plt.legend()\n","    \n","log_plot()"]},{"cell_type":"markdown","id":"74d5a5b5","metadata":{"deletable":false,"editable":false},"source":["The log of the posterior is\n","$$\\log \\left(\\Prob(y_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given y_i)\\right)\n","      = \\log\\Prob(y_i) + \\sum_{j=1}^m \\log\\Prob(x_j \\given y_i)$$\n","so that the calculation now involves the sum of a bunch of numbers rather than the product. In practice, this computation is much more robust.\n","\n","> A log-of-probability value is referred to, colloquially if not quite accurately, as a *logit*, because of a resemblance to the values of [the logit function](https://en.wikipedia.org/wiki/Logit).\n","\n","Calculate the log of the posterior for Madison for the sample text by summing up all of the pertinent logits, and similarly for Hamilton. Use the base 2 logarithm.\n","<!--\n","BEGIN QUESTION\n","name: log_posteriors\n","-->"]},{"cell_type":"code","execution_count":24,"id":"cb4d9152","metadata":{},"outputs":[],"source":["#TODO - Calculate the log of the posterior for Madison by summing up all \n","#       of the pertinent parts.\n","from math import log2\n","log_posterior_madison = log2(prior_madison) + log2(prob_whilst_madison) + (2*log2(308/(308+7+32+12)))\n","log_posterior_hamilton = log2(prior_hamilton) + (log2(prob_on_hamilton)*2) + log2(1/(390+377+369+1))"]},{"cell_type":"code","execution_count":25,"id":"0c76b6f2","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "],"text/plain":["\n","    All tests passed!\n","    "]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["grader.check(\"log_posteriors\")"]},{"cell_type":"code","execution_count":26,"id":"2eda749d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Madison  log posterior:   -7.482\n","Hamilton log posterior:  -13.610\n"]}],"source":["print(f\"Madison  log posterior: {log_posterior_madison:8.3f}\\n\"\n","      f\"Hamilton log posterior: {log_posterior_hamilton:8.3f}\")"]},{"cell_type":"markdown","id":"fffbfbfc","metadata":{"deletable":false,"editable":false},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** Which one of the two is larger? Does this accord with your expectation?\n","<!--\n","BEGIN QUESTION\n","name: open_response_posterior\n","manual: true\n","-->"]},{"attachments":{},"cell_type":"markdown","id":"acaa8709","metadata":{},"source":["Madison is larger. This corresponds with the results we got before the log."]},{"cell_type":"markdown","id":"1b4886ac","metadata":{"deletable":false,"editable":false},"source":["<!-- END QUESTION -->\n","\n","<!-- BEGIN QUESTION -->\n","\n","# Lab debrief\n","\n","**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n","\n","* Was the lab too long or too short?\n","* Were the readings appropriate for the lab? \n","* Was it clear (at least after you completed the lab) what the points of the exercises were? \n","* Are there additions or changes you think would make the lab better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"attachments":{},"cell_type":"markdown","id":"98abbfd0","metadata":{},"source":["a) It was exactly the same\n","b) Yes it was relevent\n","c)Yes\n","d)Nope"]},{"cell_type":"markdown","id":"4b5d02b5","metadata":{},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# End of lab 3"]},{"cell_type":"markdown","id":"e7bbbb5d","metadata":{"deletable":false,"editable":false},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","execution_count":27,"id":"b87f11c1","metadata":{"deletable":false,"editable":false},"outputs":[{"data":{"text/html":["<p><strong>likelihoods:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>log_posteriors:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>posteriors:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>priors:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>prob_on_hamilton:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>prob_whilst_madison:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n"],"text/plain":["likelihoods:\n","\n","    All tests passed!\n","    \n","\n","log_posteriors:\n","\n","    All tests passed!\n","    \n","\n","posteriors:\n","\n","    All tests passed!\n","    \n","\n","priors:\n","\n","    All tests passed!\n","    \n","\n","prob_on_hamilton:\n","\n","    All tests passed!\n","    \n","\n","prob_whilst_madison:\n","\n","    All tests passed!\n","    \n"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["grader.check_all()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"title":"CS236299 Lab 1-3: Naive Bayes classification"},"nbformat":4,"nbformat_minor":5}
