{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHmUjyQ2kgM4",
        "outputId": "e316baad-04f1-4544-9aa7-05728cf672de"
      },
      "id": "yHmUjyQ2kgM4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (3.8.1)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.0.0+cu118)\n",
            "Collecting otter-grader==1.0.0\n",
            "  Downloading otter_grader-1.0.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.0/164.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas==1.5.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (1.5.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (5.8.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (67.7.2)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (6.5.4)\n",
            "Collecting pdfkit\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (7.34.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (4.65.0)\n",
            "Collecting docker\n",
            "  Downloading docker-6.1.1-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 3)) (6.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r requirements.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 1)) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 1)) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->-r requirements.txt (line 2)) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->-r requirements.txt (line 2)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9.0->-r requirements.txt (line 2)) (16.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (23.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 6)) (2.27.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (9.0.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 7)) (2023.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3->-r requirements.txt (line 5)) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 6)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 6)) (1.26.15)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r requirements.txt (line 3)) (1.5.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (5.7.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (2.14.0)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (3.0.38)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader==1.0.0->-r requirements.txt (line 3)) (2.1.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (4.11.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (6.0.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.7.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (4.9.2)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (5.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.7.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (1.2.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r requirements.txt (line 3)) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r requirements.txt (line 3)) (2.16.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.2.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (0.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 3)) (23.2.1)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=6baad3e4e129a9bfe9627081c040cd2ff548151f233407d241265e6f7b7c0bb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, tokenizers, pdfkit, xxhash, PyPDF2, multidict, jedi, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, docker, aiosignal, transformers, aiohttp, otter-grader, datasets\n",
            "Successfully installed PyPDF2-3.0.1 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 docker-6.1.1 frozenlist-1.3.3 huggingface-hub-0.14.1 jedi-0.18.2 multidict-6.0.4 multiprocess-0.70.14 otter-grader-1.0.0 pdfkit-1.0.0 responses-0.18.0 tokenizers-0.13.3 transformers-4.28.1 wget-3.2 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9c20f0c5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c20f0c5",
        "outputId": "9cff083b-a6f4-4e60-ef0d-6ffe7412faa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab2-1.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f017a9aa",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f017a9aa"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "ef4c3c8a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ef4c3c8a"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1938d59e",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [
          "remove_for_latex"
        ],
        "id": "1938d59e"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718a78a5",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "718a78a5"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 2-1 – Language modeling with $n$-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae0b80b",
      "metadata": {
        "id": "9ae0b80b"
      },
      "source": [
        "We turn from tasks that _classify_ texts – mapping texts into a finite set of classes – to tasks that _model_ texts by providing a full probability distribution over texts (or providing a similar scoring metric). Such _language models_ attempt to answer the question \"How likely is a token sequence to be generated as an instance of the language?\".\n",
        "\n",
        "We'll start with $n$-gram language models. Given a token sequence $\\vect{w} = w_1, w_2, \\ldots, w_N$, its probability $\\Prob(w_1, w_2, \\ldots, w_N)$ can be calculated using the chain rule of probability:\n",
        "\n",
        "$$\\Prob(A, B \\given \\theta)= \\Prob(A \\given \\theta) \\cdot \\Prob(B \\given A, \\theta) $$\n",
        "\n",
        "Thus, \n",
        "\n",
        "\\begin{align}\n",
        "\\Prob(w_1, w_2, \\ldots, w_N) & = \\Prob(w_1) \\cdot \\Prob(w_2, \\ldots, w_N \\given w_1) \\\\\n",
        "& = \\Prob(w_1) \\cdot \\Prob(w_2 \\given w_1) \\cdot \\Prob(w_3, \\ldots, w_N \\given w_1, w_2) \\\\\n",
        "& \\cdots \\\\\n",
        "& = \\prod_{i=1}^N \\Prob (w_i \\given w_1, \\cdots, w_{i-1}) \\\\\n",
        "& \\approx \\prod_{i=1}^N \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})\\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "In the last step, we replace the probability $\\Prob (w_i \\given w_1, \\cdots, w_{i-1})$, which conditions $w_i$ on _all_ of the preceding tokens, with $\\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})$, which conditions $w_i$ only on the $n-1$ preceding tokens. We call the $n-1$ preceding tokens ($w_{i-n+1}, \\cdots, w_{i-1}$) the _context_ and $w_i$ the _target_. Taken together, these $n$ tokens form an $n$-gram, hence the term _$n$-gram model_.\n",
        "\n",
        "In this lab you'll work with $n$-gram models: generating them, sampling from them, and scoring held-out texts according to them. We'll find some problems with $n$-gram models as language models:\n",
        "\n",
        "1. They are profligate with memory.\n",
        "2. They are sensitive to very limited context.\n",
        "3. They don't generalize well across similar words.\n",
        "\n",
        "In the next lab, we'll explore neural models to address these failings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20290aa1",
      "metadata": {
        "id": "20290aa1"
      },
      "source": [
        "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
        "\n",
        "* [`itertools.product`](https://docs.python.org/3.8/library/itertools.html#itertools.product)\n",
        "* [`list`](https://docs.python.org/3/library/functions.html#func-list)\n",
        "* [`tuple`](https://docs.python.org/3/library/functions.html#func-tuple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091ae3b5",
      "metadata": {
        "id": "091ae3b5"
      },
      "source": [
        "# Preparation – Loading packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3de3a8a4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3de3a8a4"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import wget\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from sys import getsizeof\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True) # this module is used to tokenize the text\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 1234\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "143b71f0",
      "metadata": {
        "id": "143b71f0"
      },
      "outputs": [],
      "source": [
        "# Some utilities to manipulate the corpus\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Strips #comments and empty lines from a string\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        line = line.strip()              # trim whitespace\n",
        "        line = re.sub('#.*$', '', line)  # trim comments\n",
        "        if line != '':                   # drop blank lines\n",
        "            result.append(line)\n",
        "    return result\n",
        "\n",
        "def nltk_normpunc_tokenize(str):\n",
        "    return nltk.tokenize.word_tokenize(str.lower())\n",
        "\n",
        "\n",
        "def geah_tokenize(lines):\n",
        "    \"\"\"Specialized tokenizer for GEaH corpus handling speaker IDs\"\"\"\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        # tokenize\n",
        "        tokens = nltk_normpunc_tokenize(line)\n",
        "        # revert the speaker ID token\n",
        "        if tokens[0] == \"sam\":\n",
        "            tokens[0] = \"SAM:\"\n",
        "        elif tokens[0] == \"guy\":\n",
        "            tokens[0] = \"GUY:\"\n",
        "        else:\n",
        "            raise ValueError(\"format problem - bad speaker ID\")\n",
        "        # add a start of sentence token\n",
        "        result += [\"<s>\"] + tokens\n",
        "    return result\n",
        "                    \n",
        "def postprocess(tokens):\n",
        "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n",
        "    return ' '.join(tokens)\\\n",
        "              .replace(\"<s> \", \"\\n\")\n",
        "\n",
        "# Read the GEaH data and preprocess into training and test streams of tokens\n",
        "geah_filename = (\"https://github.com/nlp-236299/data/raw/master/Seuss/\"\n",
        "                 \"seuss - 1960 - green eggs and ham.txt\")\n",
        "os.makedirs('data', exist_ok=True)\n",
        "wget.download(geah_filename, out=\"data/\")\n",
        "\n",
        "def split(list, portions, offset):\n",
        "    \"\"\"Splits `list` into a \"large\" and a \"small\" part, returning them as a pair.\n",
        "    \n",
        "    The two parts are formed by partitioning `list` into `portions` disjoint pieces.\n",
        "    The small part is the piece at index `offset`; the large part is the remainder.\n",
        "    \"\"\"\n",
        "    return ([list[i] for i in range(0, len(list)) if i % portions != offset],\n",
        "            [list[i] for i in range(0, len(list)) if i % portions == offset])\n",
        "\n",
        "with open(\"data/seuss - 1960 - green eggs and ham.txt\", 'r') as fin:\n",
        "    lines = preprocess(fin.read())\n",
        "    train_lines, test_lines = split(lines, 12, 0)\n",
        "    train_tokens = geah_tokenize(train_lines)\n",
        "    test_tokens = geah_tokenize(test_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e0ded6",
      "metadata": {
        "id": "e1e0ded6"
      },
      "source": [
        "We've already loaded in the text of _Green Eggs and Ham_ for you and split it (about 90%/10%) into two token sequences, `train_tokens` and `test_tokens`. Here's a preview:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a75ef55a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a75ef55a",
        "outputId": "766a389d-c875-44e0-82f4-f1dd36e199d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'SAM:', 'i', 'am', 'sam', '.', '<s>', 'SAM:', 'sam', 'i', 'am', '.', '<s>', 'GUY:', 'that', 'sam-i-am', '!', '<s>', 'GUY:', 'that', 'sam-i-am', '!', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'that', 'sam-i-am', '!', '<s>', 'SAM:', 'do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '?', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'them', ',', 'sam-i-am']\n",
            "\n",
            "SAM: i am sam . \n",
            "SAM: sam i am . \n",
            "GUY: that sam-i-am ! \n",
            "GUY: that sam-i-am ! \n",
            "GUY: i do not like that sam-i-am ! \n",
            "SAM: do you like green eggs and ham ? \n",
            "GUY: i do not like them , sam-i-am\n"
          ]
        }
      ],
      "source": [
        "print(train_tokens[:50])\n",
        "print(postprocess(train_tokens[:50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a69f9cff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a69f9cff",
        "outputId": "0a7f4657-31ff-48c7-a18d-b4c6aae46c36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'SAM:', 'i', 'am', 'sam', '.', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', '<s>', 'GUY:', 'not', 'in', 'a', 'box', '.', '<s>', 'SAM:', 'eat', 'them', '!', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'them', 'with', 'a', 'mouse', '.', '<s>', 'GUY:', 'not', 'in', 'a', 'car', '!', '<s>', 'SAM:', 'in']\n",
            "\n",
            "SAM: i am sam . \n",
            "GUY: i do not like green eggs and ham . \n",
            "GUY: not in a box . \n",
            "SAM: eat them ! \n",
            "GUY: i do not like them with a mouse . \n",
            "GUY: not in a car ! \n",
            "SAM: in\n"
          ]
        }
      ],
      "source": [
        "print(test_tokens[:50])\n",
        "print(postprocess(test_tokens[:50]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4739aa09",
      "metadata": {
        "id": "4739aa09"
      },
      "source": [
        "We extract the vocabulary from the training text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6ac42697",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ac42697",
        "outputId": "6d8b9bc6-0c51-4c95-e658-5c3c1f028aea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SAM:', '.', 'could', 'house', 'try', '?', 'with', 'fox', 'train', 'car', 'may', 'the', 'that', 'thank', 'here', 'anywhere', 'GUY:', 'i', 'tree', 'boat', 'let', 'on', 'eat', 'green', 'will', 'sam', 'eggs', 'see', 'say', 'box', '<s>', ',', 'would', 'them', '...', 'sam-i-am', 'do', 'they', 'am', 'dark', 'are', 'be', 'like', 'if', 'rain', 'not', 'in', 'good', '!', 'or', 'a', 'mouse', 'and', 'there', 'goat', 'ham', 'you', 'me', 'so']\n"
          ]
        }
      ],
      "source": [
        "# Extract vocabulary from dataset\n",
        "vocabulary = list(set(train_tokens))\n",
        "print(vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c68ecec",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3c68ecec"
      },
      "source": [
        "# Generating $n$-grams\n",
        "\n",
        "The $n$-grams in a text are the contiguous subsequences of $n$ tokens. (We'll implement them as Python tuples.) In theory, any sequence of $n$ tokens is a potential $n$-gram type. Let's generate a list of all the possible $n$-gram types over a vocabulary. (Notice how the type/token distinction is useful for talking about $n$-grams, just as it is for words.)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: all_ngrams\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "ee014aaa",
      "metadata": {
        "id": "ee014aaa"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def all_ngrams(vocabulary, n):\n",
        "    \"\"\"Returns a list of all `n`-long *tuples* of elements of the `vocabulary`.\n",
        "    \n",
        "    For instance,  \n",
        "        >>> all_ngrams([\"one\", \"two\"], 3)\n",
        "        [('one', 'one', 'one'),\n",
        "         ('one', 'one', 'two'),\n",
        "         ('one', 'two', 'one'),\n",
        "         ('one', 'two', 'two'),\n",
        "         ('two', 'one', 'one'),\n",
        "         ('two', 'one', 'two'),\n",
        "         ('two', 'two', 'one'),\n",
        "         ('two', 'two', 'two')]\n",
        "         \n",
        "    Order of returned list is not specified or guaranteed.\n",
        "    When `n` is 0, returns `[()]`.\n",
        "    \"\"\"\n",
        "    return list(itertools.product(*([vocabulary]*n)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "955db0ec",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "955db0ec",
        "outputId": "7465848b-4872-44a0-805b-f39bf846a791"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "grader.check(\"all_ngrams\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "139789e9",
      "metadata": {
        "id": "139789e9"
      },
      "source": [
        "We can generate a list of all of the $n$-grams (tokens, not types) in a text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "baec7700",
      "metadata": {
        "id": "baec7700"
      },
      "outputs": [],
      "source": [
        "def ngrams(tokens, n):\n",
        "    \"\"\"Returns a list of all `n`-gram instances in a list of `tokens`, in order.\n",
        "    \n",
        "    For instance, \n",
        "    \n",
        "    >>> ngrams(nltk_normpunc_tokenize('I am Sam! Sam I am.'), 3)\n",
        "    [('i', 'am', 'sam'),\n",
        "     ('am', 'sam', '!'),\n",
        "     ('sam', '!', 'sam'),\n",
        "     ('!', 'sam', 'i'),\n",
        "     ('sam', 'i', 'am'),\n",
        "     ('i', 'am', '.')]\n",
        "    \"\"\"\n",
        "    return [tuple(tokens[i : i + n])\n",
        "            for i in range(0, len(tokens) - n + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "cc77430b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc77430b",
        "outputId": "e992bfe6-9a89-4788-f374-f8b3e9117558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'SAM:', 'i', 'am', 'sam', '.']\n",
            "[('<s>', 'SAM:', 'i'), ('SAM:', 'i', 'am'), ('i', 'am', 'sam'), ('am', 'sam', '.')]\n"
          ]
        }
      ],
      "source": [
        "print (train_tokens[:6])\n",
        "print (ngrams(train_tokens[:6], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8147d8",
      "metadata": {
        "id": "2d8147d8"
      },
      "source": [
        "# Counting $n$-grams\n",
        "\n",
        "We conceptualize an $n$-gram as having two parts:\n",
        "\n",
        "* The _context_ is the first $n-1$ tokens in the $n$-gram.\n",
        "* The _target_ is the final token in the $n$-gram.\n",
        "\n",
        "An $n$-gram language model specifies a probability for each $n$-gram type. We'll implement a model as a 2-D dictionary, indexed first by context and then by target, providing the probability for the $n$-gram.\n",
        "\n",
        "We start by generating a similar data structure for counting up the $n$-grams in a token sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b7a14493",
      "metadata": {
        "id": "b7a14493"
      },
      "outputs": [],
      "source": [
        "def ngram_counts(vocabulary, tokens, n):\n",
        "    \"\"\"Returns a dictionary of counts of the `n`-grams in `tokens`.\n",
        "    \n",
        "    The dictionary is structured with first index by (n-1)-gram context\n",
        "    and second index by the final target token.\n",
        "    \"\"\"\n",
        "    context_dict = defaultdict(lambda: defaultdict(int))\n",
        "    # zero all ngrams\n",
        "    for context in all_ngrams(vocabulary, n - 1):\n",
        "        for target in vocabulary:\n",
        "            context_dict[context][target] = 0\n",
        "    # add counts for attested ngrams\n",
        "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
        "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
        "    return context_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5684bc6a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5684bc6a"
      },
      "source": [
        "Use the `ngram_counts` function to generate count data structures for unigrams, bigrams, and trigrams for the _Green Eggs and Ham_ training text.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: ngram_counts\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "a4U8i208ofXB"
      },
      "id": "a4U8i208ofXB"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "a162effe",
      "metadata": {
        "id": "a162effe"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "unigram_counts = ngram_counts(set(train_tokens),train_tokens,1)\n",
        "bigram_counts = ngram_counts(set(train_tokens),train_tokens,2)\n",
        "trigram_counts = ngram_counts(set(train_tokens),train_tokens,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "284da456",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "284da456",
        "outputId": "85f21202-4661-4c70-dcb1-7f25a5843db3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "grader.check(\"ngram_counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8cf1971",
      "metadata": {
        "id": "c8cf1971"
      },
      "source": [
        "Check your work by examining the total count of unigrams, bigrams, and trigrams. Do the numbers make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "cd073b55",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd073b55",
        "outputId": "31bf4b64-7f08-4cb1-fbcb-713b12b782e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:     1145\n",
            "Unigrams:     59\n",
            "Bigrams:    3481\n",
            "Trigrams: 205379\n"
          ]
        }
      ],
      "source": [
        "# Calculate total counts of tokens, unigrams, bigrams, and trigrams\n",
        "token_count = len(train_tokens)\n",
        "unigram_count = sum(len(unigram_counts[cntxt]) for cntxt in unigram_counts)\n",
        "bigram_count = sum(len(bigram_counts[cntxt]) for cntxt in bigram_counts)\n",
        "trigram_count = sum(len(trigram_counts[cntxt]) for cntxt in trigram_counts)               \n",
        "\n",
        "# Report on the totals\n",
        "print(f\"Tokens:   {token_count:6}\\n\"\n",
        "      f\"Unigrams: {unigram_count:6}\\n\"\n",
        "      f\"Bigrams:  {bigram_count:6}\\n\"\n",
        "      f\"Trigrams: {trigram_count:6}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3c9f16",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fa3c9f16"
      },
      "source": [
        "## Calculating $n$-gram probabilities\n",
        "\n",
        "We can convert the counts into a probability model by _normalizing_ the counts. Given an $n$-gram type $w_1, w_2, \\ldots, w_n$, instead of storing the count $\\cnt{w_1, w_2, \\ldots, w_n}$, we store an estimate of the probability \n",
        "\n",
        "\\begin{align*}\n",
        "  \\Pr(w_n \\given w_1, w_2, \\ldots, w_{n-1})\n",
        "  & \\approx \\frac{\\cnt{w_1, w_2, \\ldots, w_n}}{\\cnt{w_1, w_2, \\ldots, w_{n-1}}} \\\\\n",
        "  & = \\frac{\\cnt{w_1, w_2, \\ldots, w_n}}{\\sum_{w'} \\cnt{w_1, w_2, \\ldots, w_{n-1}, w'}}\n",
        "\\end{align*}\n",
        "\n",
        "that is, the ratio of the count of the $n$-gram and the sum of the counts of all $n$-grams with the same context. Fortunately, all of those counts are already stored in the count data structures we've already built. \n",
        "\n",
        "Write a function that takes an $n$-gram count data structure and returns an $n$-gram probability data structure. As with the counts, the probabilities should be stored indexed first by context and then by target.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: ngram_model\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "35b2bf50",
      "metadata": {
        "id": "35b2bf50"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def ngram_model(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
        "       provided `ngram-counts` dictionary\n",
        "    \"\"\"\n",
        "    prob_dict = defaultdict(lambda: defaultdict(int))\n",
        "    # zero all ngrams\n",
        "    for context in ngram_counts:\n",
        "        for target in ngram_counts[context]:\n",
        "          sum_of_targets = sum([ngram_counts[context][w] for w in ngram_counts[context]])\n",
        "          if sum_of_targets == 0:\n",
        "            prob_dict[context][target] = 0\n",
        "          else:\n",
        "            prob_dict[context][target] = ngram_counts[context][target]/sum([ngram_counts[context][w] for w in ngram_counts[context]])\n",
        "    return prob_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "980fcd93",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "980fcd93",
        "outputId": "aca56c7d-8f0e-41fb-dc55-5793916173d5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "grader.check(\"ngram_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34fcbf6f",
      "metadata": {
        "id": "34fcbf6f"
      },
      "source": [
        "We can now build some $n$-gram models – unigram, bigram, and trigram – based on the counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "b493a253",
      "metadata": {
        "id": "b493a253"
      },
      "outputs": [],
      "source": [
        "unigram_model = ngram_model(unigram_counts)\n",
        "bigram_model = ngram_model(bigram_counts)\n",
        "trigram_model = ngram_model(trigram_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7877f449",
      "metadata": {
        "id": "7877f449"
      },
      "source": [
        "# Space considerations\n",
        "\n",
        "For the most part, we aren't too concerned about matters of time or space efficiency, though these are crucial issues in the engineering of NLP systems. But the size of $n$-gram models merits consideration, looking especially at their size as $n$ grows. We can use Python's [`sys.getsizeof`](https://docs.python.org/3/library/sys.html#sys.getsizeof) function to get a rough sense of the size of the models we've been working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "98765079",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98765079",
        "outputId": "ecf810d5-9e14-469a-c4f6-82160188be07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:    10328\n",
            "Unigrams:    240\n",
            "Bigrams:    2280\n",
            "Trigrams: 147560\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokens:   {getsizeof(train_tokens):6}\\n\"\n",
        "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n",
        "      f\"Bigrams:  {getsizeof(bigram_model):6}\\n\"\n",
        "      f\"Trigrams: {getsizeof(trigram_model):6}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cd0368",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c0cd0368"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What do these sizes tell you about the memory usage of $n$-gram models? With a larger vocabulary of, say, 10,000 words, would it be practical to run, say, 5-gram models on your laptop?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_sizes\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454c9746",
      "metadata": {
        "id": "454c9746"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719f573e",
      "metadata": {
        "id": "719f573e"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# Sampling from an $n$-gram model\n",
        "\n",
        "We have cleverly constructed the models to index by context. This allows us to sample a word given its context. For instance, in the trigram context `(\"<s>\", \"SAM:\")`, the following probability distribution captures which words can come next and with what probability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "6cbf12d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbf12d5",
        "outputId": "f459023e-e511-4c4c-a0f8-e16396e94ec4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'SAM:': 0.0,\n",
              "             '.': 0.0,\n",
              "             'could': 0.08823529411764706,\n",
              "             'house': 0.0,\n",
              "             'try': 0.08823529411764706,\n",
              "             '?': 0.0,\n",
              "             'with': 0.0,\n",
              "             'fox': 0.0,\n",
              "             'train': 0.0,\n",
              "             'car': 0.0,\n",
              "             'may': 0.0,\n",
              "             'the': 0.0,\n",
              "             'that': 0.0,\n",
              "             'thank': 0.0,\n",
              "             'here': 0.058823529411764705,\n",
              "             'anywhere': 0.0,\n",
              "             'GUY:': 0.0,\n",
              "             'i': 0.029411764705882353,\n",
              "             'tree': 0.0,\n",
              "             'boat': 0.0,\n",
              "             'let': 0.0,\n",
              "             'on': 0.0,\n",
              "             'eat': 0.029411764705882353,\n",
              "             'green': 0.0,\n",
              "             'will': 0.0,\n",
              "             'sam': 0.029411764705882353,\n",
              "             'eggs': 0.0,\n",
              "             'see': 0.0,\n",
              "             'say': 0.029411764705882353,\n",
              "             'box': 0.0,\n",
              "             '<s>': 0.0,\n",
              "             ',': 0.0,\n",
              "             'would': 0.2647058823529412,\n",
              "             'them': 0.0,\n",
              "             '...': 0.0,\n",
              "             'sam-i-am': 0.0,\n",
              "             'do': 0.029411764705882353,\n",
              "             'they': 0.0,\n",
              "             'am': 0.0,\n",
              "             'dark': 0.0,\n",
              "             'are': 0.0,\n",
              "             'be': 0.0,\n",
              "             'like': 0.0,\n",
              "             'if': 0.0,\n",
              "             'rain': 0.0,\n",
              "             'not': 0.0,\n",
              "             'in': 0.029411764705882353,\n",
              "             'good': 0.0,\n",
              "             '!': 0.0,\n",
              "             'or': 0.0,\n",
              "             'a': 0.11764705882352941,\n",
              "             'mouse': 0.0,\n",
              "             'and': 0.029411764705882353,\n",
              "             'there': 0.0,\n",
              "             'goat': 0.0,\n",
              "             'ham': 0.0,\n",
              "             'you': 0.14705882352941177,\n",
              "             'me': 0.0,\n",
              "             'so': 0.029411764705882353})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "trigram_model[(\"<s>\", \"SAM:\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d0d787",
      "metadata": {
        "id": "41d0d787"
      },
      "source": [
        "We can sample a single token according to this probability distribution. Here's one way to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "d33c9279",
      "metadata": {
        "id": "d33c9279"
      },
      "outputs": [],
      "source": [
        "def sample(model, context):\n",
        "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
        "    distribution = model[context]\n",
        "    prob_remaining = random.random()\n",
        "    for token, prob in sorted(distribution.items()):\n",
        "        if prob_remaining < prob:\n",
        "            return token\n",
        "        else:\n",
        "            prob_remaining -= prob\n",
        "    raise ValueError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b172efc6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b172efc6"
      },
      "source": [
        "We can extend the sampling to a sequence of words by updating the context as we sample each word.\n",
        "\n",
        "Define a function `sample_sequence` that performs this sampling of a sequence. It's given a model and a starting context and begins by sampling the first token based on the starting context, then updates the starting context to reflect the word just sampled, repeating the process until a specified number of tokens have been sampled.\n",
        "\n",
        "> Hint: You might find function [`list`](https://docs.python.org/3/library/functions.html#func-list) helpful for converting immutable tuples to lists, and conversely [`tuple`](https://docs.python.org/3/library/functions.html#func-tuple) helpful for converting lists to tuples.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: sample_sequence\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "4d41c3d1",
      "metadata": {
        "id": "4d41c3d1"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def sample_sequence(model, start_context, count=100):\n",
        "    \"\"\"Returns a sequence of `count` tokens sampled successively\n",
        "       from the `model` *following the `start_context`*.\n",
        "       The length of the returned list should be `count+len(start_context)`.\n",
        "    \"\"\"\n",
        "    random.seed(SEED) # for reproducibility, do not change\n",
        "    seq = list(start_context)\n",
        "    cnx = start_context\n",
        "    for c in range(count):\n",
        "      next_word = sample(model,cnx) \n",
        "      seq.append(next_word)\n",
        "      if len(cnx) != 0:\n",
        "        cnx = tuple(list(cnx)[1:]+[next_word])\n",
        "    return seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "f64abe4f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "f64abe4f",
        "outputId": "8fac55f0-dedd-4f5c-8141-afea38ecacc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "grader.check(\"sample_sequence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76978bfc",
      "metadata": {
        "id": "76978bfc"
      },
      "source": [
        "Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "28dd5485",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28dd5485",
        "outputId": "7c3d476b-fb8b-41fd-9eed-078564b36b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would anywhere ! tree with i like . not \n",
            ", on SAM: i i \n",
            "\n",
            ". ! do would , fox could i . i GUY: ham in do SAM: ? with box eggs ! do ! \n",
            "could a , i \n",
            "ham \n",
            "with not would \n",
            "GUY: , GUY: sam-i-am \n",
            "\n",
            "would \n",
            "the , a SAM: SAM: say dark not could say them anywhere not sam-i-am GUY: . \n",
            "and . eggs thank do say in in SAM: like sam-i-am \n",
            "tree GUY: \n",
            "them not or are . a , GUY: ,\n"
          ]
        }
      ],
      "source": [
        "print(postprocess(sample_sequence(unigram_model, ())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "bf82e1e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf82e1e7",
        "outputId": "50317d1e-bc9a-4659-a28a-a50974ec949f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SAM: sam ! \n",
            "SAM: try them , so good , will let me be ! \n",
            "GUY: and i would eat them here or there . \n",
            "GUY: i do not eat them anywhere . \n",
            "GUY: and ham . \n",
            "GUY: i do not , would you will eat them ! \n",
            "SAM: could not with a train ! \n",
            "GUY: i would not like them in the dark . \n",
            "GUY: i do not , sam-i-am . \n",
            "SAM: would you , sam-i-am . \n",
            "SAM: eat them on a train ! \n",
            "GUY: and ham !\n"
          ]
        }
      ],
      "source": [
        "print(postprocess(sample_sequence(bigram_model, (\"<s>\",))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "66321379",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66321379",
        "outputId": "0401e8c1-c853-4e8e-850b-6be25172ec68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SAM: you may , i will not eat green eggs and ham ? \n",
            "GUY: i do not like green eggs and ham . \n",
            "GUY: i do not like them anywhere ! \n",
            "SAM: say ! \n",
            "GUY: and i will eat them in a house . \n",
            "GUY: that sam-i-am ! \n",
            "GUY: not in a tree ! \n",
            "GUY: i do not like them in a tree . \n",
            "GUY: not in a box . \n",
            "GUY: not in the dark . \n",
            "GUY: not in the dark ! \n",
            "SAM: would you , in a car !\n"
          ]
        }
      ],
      "source": [
        "print(postprocess(sample_sequence(trigram_model, (\"<s>\", \"SAM:\"))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c169b5c7",
      "metadata": {
        "id": "c169b5c7"
      },
      "source": [
        "# Evaluating text according to an $n$-gram model\n",
        "\n",
        "## The probability metric\n",
        "\n",
        "The main point of a language model is to assign probabilities (or similar scores) to texts. For $n$-gram models, that's done according to Equation (1) at the start of the lab. Let's implement that. We define a function `probability` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the probability of the token sequence  according to the model. It merely multiplies all of the $n$-gram probabilities for all of the $n$-grams in the token sequence.\n",
        "\n",
        "> Throughout this lab, we ignore the scores of the first $n-1$ tokens as our $n$-gram model cannot score them due to the lack of context. In the next lab you will see how to solve this issue in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "d5a13148",
      "metadata": {
        "id": "d5a13148"
      },
      "outputs": [],
      "source": [
        "def probability(tokens, model, n):\n",
        "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = 1.0\n",
        "    context = tokens[0:n-1]\n",
        "    # Ignores the scores of the first n-1 tokens\n",
        "    for token in tokens[n-1:]:\n",
        "        prob = model[tuple(context)][token]\n",
        "        score *= prob\n",
        "        context = (context + [token])[1:]\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb1fb4f",
      "metadata": {
        "id": "9fb1fb4f"
      },
      "source": [
        "We test it on the test text that we held out from the training text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "6b66ec3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b66ec3b",
        "outputId": "23b98296-c227-4658-91c2-cb0b39cc6b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test probability - unigram: 6.404571e-154\n",
            "Test probability -  bigram: 9.147262e-44\n",
            "Test probability - trigram: 0.000000e+00\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test probability - unigram: {probability(test_tokens, unigram_model, 1):6e}\\n\"\n",
        "      f\"Test probability -  bigram: {probability(test_tokens, bigram_model, 2):6e}\\n\"\n",
        "      f\"Test probability - trigram: {probability(test_tokens, trigram_model, 3):6e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b9064a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b6b9064a"
      },
      "source": [
        "## The negative log probability metric\n",
        "\n",
        "Yikes, those probabilities are _really small_. Multiplying all those small numbers is likely to lead to underflow. \n",
        "\n",
        "To solve the underflow problem, we'll do our usual trick of using negative log probabilities \n",
        "\n",
        "$$ - \\log_2 \\left(\\prod_{i=1}^N \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})\\right)$$\n",
        "\n",
        "instead of probabilities.\n",
        "\n",
        "Define a function `neglogprob` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the negative log probability of the token sequence according to the model, calculating it in such a way as to avoid underflow. (You'll want to simplify the formula above before implementing it.)\n",
        "\n",
        "> Be careful when confronting zero probabilities. Taking `-math.log2(0)` raises a \"Math domain error\". Instead, you should use `math.inf` (Python's representation of infinity) as the value for the negative log of zero. This accords with our understanding that an impossible event would require infinite bits to specify.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: neglogprob\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "a41706ee",
      "metadata": {
        "id": "a41706ee"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def neglogprob(tokens, model, n):\n",
        "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
        "       according to an `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    context = tokens[0:n-1]\n",
        "    # Ignores the scores of the first n-1 tokens\n",
        "    for token in tokens[n-1:]:\n",
        "        prob = model[tuple(context)][token]\n",
        "        if prob == 0:\n",
        "          score += -math.inf\n",
        "        else:\n",
        "          score += math.log2(prob)\n",
        "        context = (context + [token])[1:]\n",
        "    return -score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "077fafc5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "077fafc5",
        "outputId": "adf73b5f-4601-4431-d96c-394fd219730e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "grader.check(\"neglogprob\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50204f6e",
      "metadata": {
        "id": "50204f6e"
      },
      "source": [
        "We compute the negative log probabilities of the test text using the different models and report on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "d2df5d2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2df5d2d",
        "outputId": "b2829057-93e1-4dcd-b2ac-c8949976a0cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test neglogprob - unigram: 508.897825\n",
            "Test neglogprob -  bigram: 142.971496\n",
            "Test neglogprob - trigram:    inf\n"
          ]
        }
      ],
      "source": [
        "unigram_test_nlp = neglogprob(test_tokens, unigram_model, 1)\n",
        "bigram_test_nlp = neglogprob(test_tokens, bigram_model, 2)\n",
        "trigram_test_nlp = neglogprob(test_tokens, trigram_model, 3)\n",
        "\n",
        "print(f\"Test neglogprob - unigram: {unigram_test_nlp:6f}\\n\"\n",
        "      f\"Test neglogprob -  bigram: {bigram_test_nlp:6f}\\n\"\n",
        "      f\"Test neglogprob - trigram: {trigram_test_nlp:6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d473ac4",
      "metadata": {
        "id": "1d473ac4"
      },
      "source": [
        "There, those numbers seem more manageable. We can even convert the neglogprobs back into probabilities as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "2cd31d13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cd31d13",
        "outputId": "40a7489e-e471-4ca4-fc4f-cc265a0ce8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test probability - unigram: 6.404571e-154\n",
            "Test probability -  bigram: 9.147262e-44\n",
            "Test probability - trigram: 0.000000e+00\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test probability - unigram: {2 ** (-unigram_test_nlp):6e}\\n\"\n",
        "      f\"Test probability -  bigram: {2 ** (-bigram_test_nlp):6e}\\n\"\n",
        "      f\"Test probability - trigram: {2 ** (-trigram_test_nlp):6e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb464f0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3fb464f0"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Why does the bigram model assign a lower neglogprob (that is, a higher probability) to the test text than the unigram model? Why does the trigram model assign a higher neglogprob (lower probability) to the test text than the other models?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_ordering\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200541c1",
      "metadata": {
        "id": "200541c1"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8998e652",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8998e652"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## The perplexity metric\n",
        "\n",
        "Another metric that is commonly used is _perplexity_. Jurafsky and Martin give a definition for perplexity as the \"inverse probability of the test set normalized by the number of words\":\n",
        "\n",
        "$$ PP(x_1, x_2, \\ldots, x_N) = \n",
        "     \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})}}\n",
        "$$\n",
        "\n",
        "Define a function `perplexity` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the perplexity of the token sequence according to the model, calculating it in such a way as to avoid underflow. (By now you're smart enough to realize that you'll want to carry out most of that calculation inside a $\\log$.)\n",
        "\n",
        "> Remember that we ignored the scores of the first n-1 tokens, what should the number of words `N` be?\n",
        "\n",
        "> Hint: Use the `neglogprob` function you defined above.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: perplexity\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "e369baf0",
      "metadata": {
        "id": "e369baf0"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def perplexity(tokens, model, n):\n",
        "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    return (2**neglogprob(tokens, model, n))**(1/n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "1306c196",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "1306c196",
        "outputId": "5da676fe-55b3-4d8d-b8a6-933ffb065adb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "grader.check(\"perplexity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b443e9d8",
      "metadata": {
        "id": "b443e9d8"
      },
      "source": [
        "We can look at the perplexity of the test sample according to each of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "febff32f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "febff32f",
        "outputId": "b2295416-759b-402a-e7a0-9cc0624921ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 1561384880486289339819131051754827570714489841943140954171943471901884515166392261714615593378336875792743182600807380368282348408108507356929559594270720.000\n",
            "Test perplexity -  bigram: 3306392681748722352128.000\n",
            "Test perplexity - trigram: inf\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63cd2e8",
      "metadata": {
        "id": "f63cd2e8"
      },
      "source": [
        "A perplexity value of $P$ can be interpreted as a measure of a model's average uncertainty in selecting each word equivalent to selecting among $P$ equiprobable words on average. The bigram model gives a perplexity of less than 3, indicating that at each word in the sentence, the model is acting as if selecting among (slightly less than) three equiprobable words.\n",
        "\n",
        "For comparison, state of the art $n$-gram language models for more representative English text achieve perplexities of about 250."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9cab701",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b9cab701"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Smoothing $n$-gram language models\n",
        "\n",
        "> **This section is more open-ended in nature.**\n",
        "\n",
        "The models we've been using have lots of zero-probability $n$-grams. Essentially any $n$-gram that doesn't appear in the training text is imputed a probability of zero, which means that any sentence that contains that $n$-gram will also be given a zero probability. Clearly this is not an accurate estimate.\n",
        "\n",
        "There are many ways to _smooth_ $n$-gram models, just as you smoothed classification models in earlier labs. The simplest is probably add-$\\delta$ smoothing. \n",
        "\n",
        "$$ \\Prob(w_i \\given w_1 \\ldots w_{i-1})\n",
        "  \\approx \\frac{\\cnt{w_1, w_2, \\ldots, w_n} + \\delta}{\\cnt{w_1, w_2, \\ldots, w_{n-1}} + \\delta \\cdot |V|}\n",
        "$$\n",
        "\n",
        "Another useful method is to interpolate multiple $n$-gram models, for instance, estimating probabilities as an interpolation of trigram, bigram, and unigram models.\n",
        "\n",
        "$$ \\Prob(w_i \\given w_1 \\ldots w_{i-1}) \\approx\n",
        "     \\lambda_2 \\Prob(w_i \\given w_{i-2}, w_{i-1}) \n",
        "     + \\lambda_1 \\Prob(w_i \\given w_{i-1}) \n",
        "     + (1 - \\lambda_1 - \\lambda_2) \\Prob(w_i)\n",
        "$$\n",
        "\n",
        "Finally, a method called _backoff_ uses higher-order $n$-gram probabilities where available, \"backing off\" to lower order where necessary.\n",
        "\n",
        "$$\n",
        "\\Prob(w_i \\given w_1 \\ldots w_{i-1}) \\approx \\begin{cases}\n",
        "    \\Prob(w_i \\given w_{i-2}, w_{i-1}) & \\mbox{if $\\Prob(w_i \\given w_{i-2}, w_{i-1}) > 0$}\\\\\n",
        "    \\Prob(w_i \\given w_{i-1})          & \\mbox{if $\\Prob(w_i \\given w_{i-2}, w_{i-1}) = 0$ and $\\Prob(w_i \\given w_{i-1}) > 0$}\\\\\n",
        "    \\Prob(w_i)                         & \\mbox{otherwise}\n",
        "  \\end{cases}\n",
        "$$\n",
        "\n",
        "Define a function `ngram_model_smoothed`, like the `ngram_model` function from above, but implementing one of these smoothing methods. Compare its perplexity on some sample text to the unsmoothed model. \n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_smoothed_model\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccd6bc6",
      "metadata": {
        "id": "1ccd6bc6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#TODO\n",
        "Place your definition of `ngram_model_smoothed` and whatever other testing \n",
        "of it you'd like to do in this and subsequent cells.\n",
        "\"\"\"\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c71003",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "04c71003"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2570a60",
      "metadata": {
        "id": "f2570a60"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49448a5",
      "metadata": {
        "id": "b49448a5"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of Lab 2-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb011ed",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2fb011ed"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa6ede2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1aa6ede2"
      },
      "outputs": [],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "otter-latest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 2-1: Language modeling with n-grams",
    "vscode": {
      "interpreter": {
        "hash": "4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}